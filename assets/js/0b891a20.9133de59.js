"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[2868],{1833:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>a,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=n(5893),s=n(1151);const r={sidebar_label:"retrieve_user_proxy_agent",title:"agentchat.contrib.retrieve_user_proxy_agent"},o=void 0,l={id:"reference/agentchat/contrib/retrieve_user_proxy_agent",title:"agentchat.contrib.retrieve_user_proxy_agent",description:"RetrieveUserProxyAgent",source:"@site/docs/reference/agentchat/contrib/retrieve_user_proxy_agent.md",sourceDirName:"reference/agentchat/contrib",slug:"/reference/agentchat/contrib/retrieve_user_proxy_agent",permalink:"/autogen/docs/reference/agentchat/contrib/retrieve_user_proxy_agent",draft:!1,unlisted:!1,editUrl:"https://github.com/microsoft/autogen/edit/main/website/docs/reference/agentchat/contrib/retrieve_user_proxy_agent.md",tags:[],version:"current",frontMatter:{sidebar_label:"retrieve_user_proxy_agent",title:"agentchat.contrib.retrieve_user_proxy_agent"},sidebar:"referenceSideBar",previous:{title:"retrieve_assistant_agent",permalink:"/autogen/docs/reference/agentchat/contrib/retrieve_assistant_agent"},next:{title:"society_of_mind_agent",permalink:"/autogen/docs/reference/agentchat/contrib/society_of_mind_agent"}},a={},c=[{value:"RetrieveUserProxyAgent",id:"retrieveuserproxyagent",level:2},{value:"__init__",id:"__init__",level:3},{value:"retrieve_docs",id:"retrieve_docs",level:3},{value:"generate_init_message",id:"generate_init_message",level:3}];function d(e){const t={a:"a",code:"code",em:"em",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.a)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(t.h2,{id:"retrieveuserproxyagent",children:"RetrieveUserProxyAgent"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:"class RetrieveUserProxyAgent(UserProxyAgent)\n"})}),"\n",(0,i.jsx)(t.h3,{id:"__init__",children:"__init__"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:'def __init__(name="RetrieveChatAgent",\n             human_input_mode: Optional[str] = "ALWAYS",\n             is_termination_msg: Optional[Callable[[Dict], bool]] = None,\n             retrieve_config: Optional[Dict] = None,\n             **kwargs)\n'})}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Arguments"}),":"]}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.code,{children:"name"})," ",(0,i.jsx)(t.em,{children:"str"})," - name of the agent."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.code,{children:"human_input_mode"})," ",(0,i.jsx)(t.em,{children:"str"}),' - whether to ask for human inputs every time a message is received.\nPossible values are "ALWAYS", "TERMINATE", "NEVER".\n',(0,i.jsxs)(t.ol,{children:["\n",(0,i.jsx)(t.li,{children:'When "ALWAYS", the agent prompts for human input every time a message is received.\nUnder this mode, the conversation stops when the human input is "exit",\nor when is_termination_msg is True and there is no human input.'}),"\n",(0,i.jsx)(t.li,{children:'When "TERMINATE", the agent only prompts for human input only when a termination message is received or\nthe number of auto reply reaches the max_consecutive_auto_reply.'}),"\n",(0,i.jsx)(t.li,{children:'When "NEVER", the agent will never prompt for human input. Under this mode, the conversation stops\nwhen the number of auto reply reaches the max_consecutive_auto_reply or when is_termination_msg is True.'}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.code,{children:"is_termination_msg"})," ",(0,i.jsx)(t.em,{children:"function"}),' - a function that takes a message in the form of a dictionary\nand returns a boolean value indicating if this received message is a termination message.\nThe dict can contain the following keys: "content", "role", "name", "function_call".']}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.code,{children:"retrieve_config"})," ",(0,i.jsx)(t.em,{children:"dict or None"})," - config for the retrieve agent.\nTo use default config, set to None. Otherwise, set to a dictionary with the following keys:\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:['task (Optional, str): the task of the retrieve chat. Possible values are "code", "qa" and "default". System\nprompt will be different for different tasks. The default value is ',(0,i.jsx)(t.code,{children:"default"}),", which supports both code and qa."]}),"\n",(0,i.jsxs)(t.li,{children:["client (Optional, chromadb.Client): the chromadb client. If key not provided, a default client ",(0,i.jsx)(t.code,{children:"chromadb.Client()"}),"\nwill be used. If you want to use other vector db, extend this class and override the ",(0,i.jsx)(t.code,{children:"retrieve_docs"})," function."]}),"\n",(0,i.jsx)(t.li,{children:"docs_path (Optional, Union[str, List[str]]): the path to the docs directory. It can also be the path to a single file,\nthe url to a single file or a list of directories, files and urls. Default is None, which works only if the collection is already created."}),"\n",(0,i.jsx)(t.li,{children:'extra_docs (Optional, bool): when true, allows adding documents with unique IDs without overwriting existing ones; when false, it replaces existing documents using default IDs, risking collection overwrite.,\nwhen set to true it enables the system to assign unique IDs starting from "length+i" for new document chunks, preventing the replacement of existing documents and facilitating the addition of more content to the collection..\nBy default, "extra_docs" is set to false, starting document IDs from zero. This poses a risk as new documents might overwrite existing ones, potentially causing unintended loss or alteration of data in the collection.'}),"\n",(0,i.jsxs)(t.li,{children:["collection_name (Optional, str): the name of the collection.\nIf key not provided, a default name ",(0,i.jsx)(t.code,{children:"autogen-docs"})," will be used."]}),"\n",(0,i.jsxs)(t.li,{children:["model (Optional, str): the model to use for the retrieve chat.\nIf key not provided, a default model ",(0,i.jsx)(t.code,{children:"gpt-4"})," will be used."]}),"\n",(0,i.jsxs)(t.li,{children:["chunk_token_size (Optional, int): the chunk token size for the retrieve chat.\nIf key not provided, a default size ",(0,i.jsx)(t.code,{children:"max_tokens * 0.4"})," will be used."]}),"\n",(0,i.jsxs)(t.li,{children:["context_max_tokens (Optional, int): the context max token size for the retrieve chat.\nIf key not provided, a default size ",(0,i.jsx)(t.code,{children:"max_tokens * 0.8"})," will be used."]}),"\n",(0,i.jsxs)(t.li,{children:['chunk_mode (Optional, str): the chunk mode for the retrieve chat. Possible values are\n"multi_lines" and "one_line". If key not provided, a default mode ',(0,i.jsx)(t.code,{children:"multi_lines"})," will be used."]}),"\n",(0,i.jsx)(t.li,{children:'must_break_at_empty_line (Optional, bool): chunk will only break at empty line if True. Default is True.\nIf chunk_mode is "one_line", this parameter will be ignored.'}),"\n",(0,i.jsxs)(t.li,{children:["embedding_model (Optional, str): the embedding model to use for the retrieve chat.\nIf key not provided, a default model ",(0,i.jsx)(t.code,{children:"all-MiniLM-L6-v2"})," will be used. All available models\ncan be found at ",(0,i.jsx)(t.code,{children:"https://www.sbert.net/docs/pretrained_models.html"}),". The default model is a\nfast model. If you want to use a high performance model, ",(0,i.jsx)(t.code,{children:"all-mpnet-base-v2"})," is recommended."]}),"\n",(0,i.jsxs)(t.li,{children:["embedding_function (Optional, Callable): the embedding function for creating the vector db. Default is None,\nSentenceTransformer with the given ",(0,i.jsx)(t.code,{children:"embedding_model"})," will be used. If you want to use OpenAI, Cohere, HuggingFace or\nother embedding functions, you can pass it here, follow the examples in ",(0,i.jsx)(t.code,{children:"https://docs.trychroma.com/embeddings"}),"."]}),"\n",(0,i.jsx)(t.li,{children:"customized_prompt (Optional, str): the customized prompt for the retrieve chat. Default is None."}),"\n",(0,i.jsxs)(t.li,{children:['customized_answer_prefix (Optional, str): the customized answer prefix for the retrieve chat. Default is "".\nIf not "" and the customized_answer_prefix is not in the answer, ',(0,i.jsx)(t.code,{children:"Update Context"})," will be triggered."]}),"\n",(0,i.jsxs)(t.li,{children:["update_context (Optional, bool): if False, will not apply ",(0,i.jsx)(t.code,{children:"Update Context"})," for interactive retrieval. Default is True."]}),"\n",(0,i.jsx)(t.li,{children:"get_or_create (Optional, bool): if True, will create/return a collection for the retrieve chat. This is the same as that used in chromadb.\nDefault is False. Will raise ValueError if the collection already exists and get_or_create is False. Will be set to True if docs_path is None."}),"\n",(0,i.jsx)(t.li,{children:'custom_token_count_function (Optional, Callable): a custom function to count the number of tokens in a string.\nThe function should take (text:str, model:str) as input and return the token_count(int). the retrieve_config["model"] will be passed in the function.\nDefault is autogen.token_count_utils.count_token that uses tiktoken, which may not be accurate for non-OpenAI models.'}),"\n",(0,i.jsxs)(t.li,{children:["custom_text_split_function (Optional, Callable): a custom function to split a string into a list of strings.\nDefault is None, will use the default function in ",(0,i.jsx)(t.code,{children:"autogen.retrieve_utils.split_text_to_chunks"}),"."]}),"\n",(0,i.jsxs)(t.li,{children:["custom_text_types (Optional, List[str]): a list of file types to be processed. Default is ",(0,i.jsx)(t.code,{children:"autogen.retrieve_utils.TEXT_FORMATS"}),".\nThis only applies to files under the directories in ",(0,i.jsx)(t.code,{children:"docs_path"}),". Explicitly included files and urls will be chunked regardless of their types."]}),"\n",(0,i.jsx)(t.li,{children:"recursive (Optional, bool): whether to search documents recursively in the docs_path. Default is True."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.code,{children:"**kwargs"})," ",(0,i.jsx)(t.em,{children:"dict"})," - other kwargs in ",(0,i.jsx)(t.a,{href:"../user_proxy_agent#__init__",children:"UserProxyAgent"}),"."]}),"\n"]}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Example"}),":"]}),"\n",(0,i.jsx)(t.p,{children:"Example of overriding retrieve_docs - If you have set up a customized vector db, and it's not compatible with chromadb, you can easily plug in it with below code."}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:'class MyRetrieveUserProxyAgent(RetrieveUserProxyAgent):\n    def query_vector_db(\n        self,\n        query_texts: List[str],\n        n_results: int = 10,\n        search_string: str = "",\n        **kwargs,\n    ) -> Dict[str, Union[List[str], List[List[str]]]]:\n        # define your own query function here\n        pass\n\n    def retrieve_docs(self, problem: str, n_results: int = 20, search_string: str = "", **kwargs):\n        results = self.query_vector_db(\n            query_texts=[problem],\n            n_results=n_results,\n            search_string=search_string,\n            **kwargs,\n        )\n\n        self._results = results\n        print("doc_ids: ", results["ids"])\n'})}),"\n",(0,i.jsx)(t.h3,{id:"retrieve_docs",children:"retrieve_docs"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:'def retrieve_docs(problem: str, n_results: int = 20, search_string: str = "")\n'})}),"\n",(0,i.jsxs)(t.p,{children:["Retrieve docs based on the given problem and assign the results to the class property ",(0,i.jsx)(t.code,{children:"_results"}),".\nIn case you want to customize the retrieval process, such as using a different vector db whose APIs are not\ncompatible with chromadb or filter results with metadata, you can override this function. Just keep the current\nparameters and add your own parameters with default values, and keep the results in below type."]}),"\n",(0,i.jsxs)(t.p,{children:['Type of the results: Dict[str, List[List[Any]]], should have keys "ids" and "documents", "ids" for the ids of\nthe retrieved docs and "documents" for the contents of the retrieved docs. Any other keys are optional. Refer\nto ',(0,i.jsx)(t.code,{children:"chromadb.api.types.QueryResult"})," as an example.\nids: List[string]\ndocuments: List[List[string]]"]}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Arguments"}),":"]}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.code,{children:"problem"})," ",(0,i.jsx)(t.em,{children:"str"})," - the problem to be solved."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.code,{children:"n_results"})," ",(0,i.jsx)(t.em,{children:"int"})," - the number of results to be retrieved. Default is 20."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.code,{children:"search_string"})," ",(0,i.jsx)(t.em,{children:"str"}),' - only docs that contain an exact match of this string will be retrieved. Default is "".']}),"\n"]}),"\n",(0,i.jsx)(t.h3,{id:"generate_init_message",children:"generate_init_message"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:'def generate_init_message(problem: str,\n                          n_results: int = 20,\n                          search_string: str = "")\n'})}),"\n",(0,i.jsx)(t.p,{children:"Generate an initial message with the given problem and prompt."}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Arguments"}),":"]}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.code,{children:"problem"})," ",(0,i.jsx)(t.em,{children:"str"})," - the problem to be solved."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.code,{children:"n_results"})," ",(0,i.jsx)(t.em,{children:"int"})," - the number of results to be retrieved."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.code,{children:"search_string"})," ",(0,i.jsx)(t.em,{children:"str"})," - only docs containing this string will be retrieved."]}),"\n"]}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Returns"}),":"]}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.code,{children:"str"})," - the generated prompt ready to be sent to the assistant agent."]}),"\n"]})]})}function h(e={}){const{wrapper:t}={...(0,s.a)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},1151:(e,t,n)=>{n.d(t,{Z:()=>l,a:()=>o});var i=n(7294);const s={},r=i.createContext(s);function o(e){const t=i.useContext(r);return i.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function l(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),i.createElement(r.Provider,{value:t},e.children)}}}]);