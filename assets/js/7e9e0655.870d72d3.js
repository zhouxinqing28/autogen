"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[6617],{61442:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>r,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>l,toc:()=>c});var t=s(85893),i=s(11151);const o={sidebar_label:"compressible_agent",title:"agentchat.contrib.compressible_agent"},a=void 0,l={id:"reference/agentchat/contrib/compressible_agent",title:"agentchat.contrib.compressible_agent",description:"CompressibleAgent",source:"@site/docs/reference/agentchat/contrib/compressible_agent.md",sourceDirName:"reference/agentchat/contrib",slug:"/reference/agentchat/contrib/compressible_agent",permalink:"/autogen/docs/reference/agentchat/contrib/compressible_agent",draft:!1,unlisted:!1,editUrl:"https://github.com/microsoft/autogen/edit/main/website/docs/reference/agentchat/contrib/compressible_agent.md",tags:[],version:"current",frontMatter:{sidebar_label:"compressible_agent",title:"agentchat.contrib.compressible_agent"},sidebar:"referenceSideBar",previous:{title:"agent_builder",permalink:"/autogen/docs/reference/agentchat/contrib/agent_builder"},next:{title:"gpt_assistant_agent",permalink:"/autogen/docs/reference/agentchat/contrib/gpt_assistant_agent"}},r={},c=[{value:"CompressibleAgent",id:"compressibleagent",level:2},{value:"__init__",id:"__init__",level:3},{value:"generate_reply",id:"generate_reply",level:3},{value:"on_oai_token_limit",id:"on_oai_token_limit",level:3},{value:"compress_messages",id:"compress_messages",level:3}];function d(e){const n={a:"a",code:"code",em:"em",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.a)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h2,{id:"compressibleagent",children:"CompressibleAgent"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class CompressibleAgent(ConversableAgent)\n"})}),"\n",(0,t.jsxs)(n.p,{children:["(CompressibleAgent will be deprecated. Refer to ",(0,t.jsx)(n.a,{href:"https://github.com/microsoft/autogen/blob/main/notebook/agentchat_capability_long_context_handling.ipynb",children:"https://github.com/microsoft/autogen/blob/main/notebook/agentchat_capability_long_context_handling.ipynb"})," for long context handling capability.) CompressibleAgent agent. While this agent retains all the default functionalities of the ",(0,t.jsx)(n.code,{children:"AssistantAgent"}),",\nit also provides the added feature of compression when activated through the ",(0,t.jsx)(n.code,{children:"compress_config"})," setting."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.code,{children:"compress_config"})," is set to False by default, making this agent equivalent to the ",(0,t.jsx)(n.code,{children:"AssistantAgent"}),".\nThis agent does not work well in a GroupChat: The compressed messages will not be sent to all the agents in the group.\nThe default system message is the same as AssistantAgent.\n",(0,t.jsx)(n.code,{children:"human_input_mode"}),' is default to "NEVER"\nand ',(0,t.jsx)(n.code,{children:"code_execution_config"})," is default to False.\nThis agent doesn't execute code or function call by default."]}),"\n",(0,t.jsx)(n.h3,{id:"__init__",children:"__init__"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def __init__(name: str,\n             system_message: Optional[str] = DEFAULT_SYSTEM_MESSAGE,\n             is_termination_msg: Optional[Callable[[Dict], bool]] = None,\n             max_consecutive_auto_reply: Optional[int] = None,\n             human_input_mode: Optional[str] = "NEVER",\n             function_map: Optional[Dict[str, Callable]] = None,\n             code_execution_config: Optional[Union[Dict, bool]] = False,\n             llm_config: Optional[Union[Dict, bool]] = None,\n             default_auto_reply: Optional[Union[str, Dict, None]] = "",\n             compress_config: Optional[Dict] = False,\n             description: Optional[str] = None,\n             **kwargs)\n'})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Arguments"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"name"})," ",(0,t.jsx)(n.em,{children:"str"})," - agent name."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"system_message"})," ",(0,t.jsx)(n.em,{children:"str"})," - system message for the ChatCompletion inference.\nPlease override this attribute if you want to reprogram the agent."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"llm_config"})," ",(0,t.jsx)(n.em,{children:"dict"})," - llm inference configuration."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"Note"})," - you must set ",(0,t.jsx)(n.code,{children:"model"})," in llm_config. It will be used to compute the token count.\nPlease refer to ",(0,t.jsx)(n.a,{href:"/docs/reference/oai/client#create",children:"OpenAIWrapper.create"}),"\nfor available options."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"is_termination_msg"})," ",(0,t.jsx)(n.em,{children:"function"}),' - a function that takes a message in the form of a dictionary\nand returns a boolean value indicating if this received message is a termination message.\nThe dict can contain the following keys: "content", "role", "name", "function_call".']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"max_consecutive_auto_reply"})," ",(0,t.jsx)(n.em,{children:"int"}),' - the maximum number of consecutive auto replies.\ndefault to None (no limit provided, class attribute MAX_CONSECUTIVE_AUTO_REPLY will be used as the limit in this case).\nThe limit only plays a role when human_input_mode is not "ALWAYS".']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"compress_config"})," ",(0,t.jsx)(n.em,{children:"dict or True/False"})," - config for compression before oai_reply. Default to False.\nYou should contain the following keys:\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:'"mode" (Optional, str, default to "TERMINATE"): Choose from ["COMPRESS", "TERMINATE", "CUSTOMIZED"].'}),"\n"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"TERMINATE"}),": terminate the conversation ONLY when token count exceeds the max limit of current model. ",(0,t.jsx)(n.code,{children:"trigger_count"})," is NOT used in this mode."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"COMPRESS"}),": compress the messages when the token count exceeds the limit."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"CUSTOMIZED"}),": pass in a customized function to compress the messages."]}),"\n"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:'"compress_function" (Optional, callable, default to None): Must be provided when mode is "CUSTOMIZED".\nThe function should takes a list of messages and returns a tuple of (is_compress_success: bool, compressed_messages: List[Dict]).'}),"\n",(0,t.jsx)(n.li,{children:'"trigger_count" (Optional, float, int, default to 0.7): the threshold to trigger compression.\nIf a float between (0, 1], it is the percentage of token used. if a int, it is the number of tokens used.'}),"\n",(0,t.jsx)(n.li,{children:'"async" (Optional, bool, default to False): whether to compress asynchronously.'}),"\n",(0,t.jsx)(n.li,{children:'"broadcast" (Optional, bool, default to True): whether to update the compressed message history to sender.'}),"\n",(0,t.jsx)(n.li,{children:'"verbose" (Optional, bool, default to False): Whether to print the content before and after compression. Used when mode="COMPRESS".'}),"\n",(0,t.jsx)(n.li,{children:'"leave_last_n" (Optional, int, default to 0): If provided, the last n messages will not be compressed. Used when mode="COMPRESS".'}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"description"})," ",(0,t.jsx)(n.em,{children:"str"})," - a short description of the agent. This description is used by other agents\n(e.g. the GroupChatManager) to decide when to call upon this agent. (Default: system_message)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"**kwargs"})," ",(0,t.jsx)(n.em,{children:"dict"})," - Please refer to other kwargs in\n",(0,t.jsx)(n.a,{href:"../conversable_agent#__init__",children:"ConversableAgent"}),"."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"generate_reply",children:"generate_reply"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"def generate_reply(\n        messages: Optional[List[Dict]] = None,\n        sender: Optional[Agent] = None,\n        exclude: Optional[List[Callable]] = None) -> Union[str, Dict, None]\n"})}),"\n",(0,t.jsx)(n.p,{children:"Adding to line 202:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"    if messages is not None and messages != self._oai_messages[sender]:\n        messages = self._oai_messages[sender]\n"})}),"\n",(0,t.jsx)(n.h3,{id:"on_oai_token_limit",children:"on_oai_token_limit"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"def on_oai_token_limit(\n        messages: Optional[List[Dict]] = None,\n        sender: Optional[Agent] = None,\n        config: Optional[Any] = None) -> Tuple[bool, Union[str, Dict, None]]\n"})}),"\n",(0,t.jsx)(n.p,{children:"(Experimental) Compress previous messages when a threshold of tokens is reached."}),"\n",(0,t.jsx)(n.p,{children:"TODO: async compress\nTODO: maintain a list for old oai messages (messages before compression)"}),"\n",(0,t.jsx)(n.h3,{id:"compress_messages",children:"compress_messages"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"def compress_messages(\n        messages: Optional[List[Dict]] = None,\n        config: Optional[Any] = None\n) -> Tuple[bool, Union[str, Dict, None, List]]\n"})}),"\n",(0,t.jsx)(n.p,{children:"Compress a list of messages into one message."}),"\n",(0,t.jsx)(n.p,{children:"The first message (the initial prompt) will not be compressed.\nThe rest of the messages will be compressed into one message, the model is asked to distinguish the role of each message: USER, ASSISTANT, FUNCTION_CALL, FUNCTION_RETURN.\nCheck out the compress_sys_msg."}),"\n",(0,t.jsx)(n.p,{children:"TODO: model used in compression agent is different from assistant agent: For example, if original model used by is gpt-4; we start compressing at 70% of usage, 70% of 8092 = 5664; and we use gpt 3.5 here max_toke = 4096, it will raise error. choosinng model automatically?"})]})}function h(e={}){const{wrapper:n}={...(0,i.a)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},11151:(e,n,s)=>{s.d(n,{Z:()=>l,a:()=>a});var t=s(67294);const i={},o=t.createContext(i);function a(e){const n=t.useContext(o);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);