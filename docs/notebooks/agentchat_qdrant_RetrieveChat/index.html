<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-notebooks/agentchat_qdrant_RetrieveChat" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.1.1">
<title data-rh="true">Using RetrieveChat with Qdrant for Retrieve Augmented Code Generation and Question Answering | AutoGen</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://microsoft.github.io/autogen/docs/notebooks/agentchat_qdrant_RetrieveChat"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Using RetrieveChat with Qdrant for Retrieve Augmented Code Generation and Question Answering | AutoGen"><meta data-rh="true" name="description" content="This notebook demonstrates the usage of QdrantRetrieveUserProxyAgent for RAG."><meta data-rh="true" property="og:description" content="This notebook demonstrates the usage of QdrantRetrieveUserProxyAgent for RAG."><link data-rh="true" rel="icon" href="/autogen/img/ag.ico"><link data-rh="true" rel="canonical" href="https://microsoft.github.io/autogen/docs/notebooks/agentchat_qdrant_RetrieveChat"><link data-rh="true" rel="alternate" href="https://microsoft.github.io/autogen/docs/notebooks/agentchat_qdrant_RetrieveChat" hreflang="en"><link data-rh="true" rel="alternate" href="https://microsoft.github.io/autogen/docs/notebooks/agentchat_qdrant_RetrieveChat" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/autogen/blog/rss.xml" title="AutoGen RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/autogen/blog/atom.xml" title="AutoGen Atom Feed">






<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" integrity="sha384-Um5gpz1odJg5Z4HAmzPtgZKdTBHZdw8S29IecapCSB31ligYPhHQZMIlWLYQGVoc" crossorigin="anonymous">
<script src="/autogen/js/custom.js" async defer="defer"></script><link rel="stylesheet" href="/autogen/assets/css/styles.d8ef5d37.css">
<script src="/autogen/assets/js/runtime~main.f6c7c06d.js" defer="defer"></script>
<script src="/autogen/assets/js/main.887e557c.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const a=new URLSearchParams(window.location.search).entries();for(var[t,e]of a)if(t.startsWith("docusaurus-data-")){var n=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(n,e)}}catch(t){}}(),document.documentElement.setAttribute("data-announcement-bar-initially-dismissed",function(){try{return"true"===localStorage.getItem("docusaurus.announcement.dismiss")}catch(t){}return!1}())</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><div class="announcementBar_mb4j" style="background-color:#fafbfc;color:#091E42" role="banner"><div class="announcementBarPlaceholder_vyr4"></div><div class="content_knG7 announcementBarContent_xLdY">What's new in AutoGen? Read <a href="/autogen/blog/2024/03/03/AutoGen-Update">this blog</a> for an overview of updates</div><button type="button" aria-label="Close" class="clean-btn close closeButton_CVFx announcementBarClose_gvF7"><svg viewBox="0 0 15 15" width="14" height="14"><g stroke="currentColor" stroke-width="3.1"><path d="M.75.75l13.5 13.5M14.25.75L.75 14.25"></path></g></svg></button></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/autogen/"><div class="navbar__logo"><img src="/autogen/img/ag.svg" alt="AutoGen" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/autogen/img/ag.svg" alt="AutoGen" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">AutoGen</b></a><a class="navbar__item navbar__link" href="/autogen/docs/Getting-Started">Docs</a><a class="navbar__item navbar__link" href="/autogen/docs/reference/agentchat/conversable_agent">API</a><a class="navbar__item navbar__link" href="/autogen/blog">Blog</a><a class="navbar__item navbar__link" href="/autogen/docs/FAQ">FAQ</a><a class="navbar__item navbar__link" href="/autogen/docs/Examples">Examples</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/autogen/docs/notebooks">Notebooks</a><a class="navbar__item navbar__link" href="/autogen/docs/Gallery">Gallery</a></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Other Languages</a><ul class="dropdown__menu"><li><a href="https://microsoft.github.io/autogen-for-net/" target="_blank" rel="noopener noreferrer" class="dropdown__link">Dotnet<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><a href="https://github.com/microsoft/autogen" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><div class="navbar__search searchBarContainer_NW3z"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div><div class="searchHintContainer_Pkmr"><kbd class="searchHint_iIMx">ctrl</kbd><kbd class="searchHint_iIMx">K</kbd></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG menuWithAnnouncementBar_GW3s"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" aria-expanded="true" href="/autogen/docs/notebooks">Notebooks</a><button aria-label="Collapse sidebar category &#x27;Notebooks&#x27;" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchat_RetrieveChat">Using RetrieveChat for Retrieve Augmented Code Generation and Question Answering</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchat_auto_feedback_from_code_execution">Task Solving with Code Generation, Execution and Debugging</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchat_capability_long_context_handling">Handling A Long Context via `TransformChatHistory`</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchat_compression">Conversations with Chat History Compression Enabled</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchat_custom_model">Agent Chat with custom model loading</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchat_function_call_async">Task Solving with Provided Tools as Functions (Asynchronous Function Calls)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchat_groupchat">Group Chat</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchat_groupchat_RAG">Group Chat with Retrieval Augmented Generation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchat_groupchat_customized">Group Chat with Customized Speaker Selection Method</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchat_groupchat_finite_state_machine">FSM - User can input speaker transition constraints</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchat_groupchat_research">Perform Research with Multi-Agent Group Chat</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchat_groupchat_stateflow">StateFlow: Build Workflows through State-Oriented Actions</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchat_groupchat_vis">Group Chat with Coder and Visualization Critic</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchat_image_generation_capability">Adding Different LLM Modalities to Exisiting Agents</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchat_lmm_gpt-4v">Engaging with Multimodal Models: GPT-4V in AutoGen</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchat_logging">Runtime Logging with AutoGen</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchat_multi_task_async_chats">Solving Multiple Tasks in a Sequence of Async Chats</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchat_multi_task_chats">Solving Multiple Tasks in a Sequence of Chats</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchat_nested_chats_chess">Nested Chats for Tool Use in Conversational Chess</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchat_nested_sequential_chats">Solving Complex Tasks with A Sequence of Nested Chats</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchat_nestedchat">Solving Complex Tasks with Nested Chats</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchat_nestedchat_optiguide">OptiGuide with Nested Chats in AutoGen</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/autogen/docs/notebooks/agentchat_qdrant_RetrieveChat">Using RetrieveChat with Qdrant for Retrieve Augmented Code Generation and Question Answering</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchat_society_of_mind">SocietyOfMindAgent</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchat_teachability">Chatting with a teachable agent</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchat_teaching">Auto Generated Agent Chat: Teaching</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchat_video_transcript_translate_with_whisper">Translating Video audio using Whisper and GPT-3.5-turbo</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchats_sequential_chats">Solving Multiple Tasks in a Sequence of Chats with Different Conversable Agent Pairs</a></li></ul></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/autogen/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/autogen/docs/notebooks"><span itemprop="name">Notebooks</span></a><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Using RetrieveChat with Qdrant for Retrieve Augmented Code Generation and Question Answering</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>Using RetrieveChat with Qdrant for Retrieve Augmented Code Generation and Question Answering</h1>
<p><a href="https://colab.research.google.com/github/microsoft/autogen/blob/main/notebook/agentchat_qdrant_RetrieveChat.ipynb" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" class="img_ev3q"></a>
<a href="https://github.com/microsoft/autogen/blob/main/notebook/agentchat_qdrant_RetrieveChat.ipynb" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github" alt="Open on GitHub" class="img_ev3q"></a></p>
<p><a href="https://qdrant.tech/" target="_blank" rel="noopener noreferrer">Qdrant</a> is a high-performance vector search
engine/database.</p>
<p>This notebook demonstrates the usage of <code>QdrantRetrieveUserProxyAgent</code>
for RAG, based on
<a href="https://colab.research.google.com/github/microsoft/autogen/blob/main/notebook/agentchat_RetrieveChat.ipynb" target="_blank" rel="noopener noreferrer">agentchat_RetrieveChat.ipynb</a>.</p>
<p>RetrieveChat is a conversational system for retrieve augmented code
generation and question answering. In this notebook, we demonstrate how
to utilize RetrieveChat to generate code and answer questions based on
customized documentations that are not present in the LLM’s training
dataset. RetrieveChat uses the <code>RetrieveAssistantAgent</code> and
<code>QdrantRetrieveUserProxyAgent</code>, which is similar to the usage of
<code>AssistantAgent</code> and <code>UserProxyAgent</code> in other notebooks (e.g.,
<a href="https://github.com/microsoft/autogen/blob/main/notebook/agentchat_auto_feedback_from_code_execution.ipynb" target="_blank" rel="noopener noreferrer">Automated Task Solving with Code Generation, Execution &amp;
Debugging</a>).</p>
<p>We’ll demonstrate usage of RetrieveChat with Qdrant for code generation
and question answering w/ human feedback.</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>Requirements</div><div class="admonitionContent_BuS1"><p>Some extra dependencies are needed for this notebook, which can be installed via pip:</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token plain">pip install &quot;pyautogen[retrievechat]&gt;=0.2.3&quot; &quot;flaml[automl]&quot; &quot;qdrant_client[fastembed]&quot;</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>For more information, please refer to the <a href="/autogen/docs/installation/">installation guide</a>.</p></div></div>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token operator" style="color:rgb(137, 221, 255)">%</span><span class="token plain">pip install </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;pyautogen[retrievechat]&gt;=0.2.3&quot;</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;flaml[automl]&quot;</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;qdrant_client[fastembed]&quot;</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: pyautogen&gt;=0.2.3 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from pyautogen[retrievechat]&gt;=0.2.3) (0.2.3)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: flaml[automl] in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (2.1.1)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: qdrant_client[fastembed] in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (1.7.0)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: diskcache in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from pyautogen&gt;=0.2.3-&gt;pyautogen[retrievechat]&gt;=0.2.3) (5.6.3)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: openai&gt;=1.3 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from pyautogen&gt;=0.2.3-&gt;pyautogen[retrievechat]&gt;=0.2.3) (1.6.1)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: pydantic&lt;3,&gt;=1.10 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from pyautogen&gt;=0.2.3-&gt;pyautogen[retrievechat]&gt;=0.2.3) (2.5.3)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: python-dotenv in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from pyautogen&gt;=0.2.3-&gt;pyautogen[retrievechat]&gt;=0.2.3) (1.0.0)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: termcolor in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from pyautogen&gt;=0.2.3-&gt;pyautogen[retrievechat]&gt;=0.2.3) (2.4.0)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: tiktoken in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from pyautogen&gt;=0.2.3-&gt;pyautogen[retrievechat]&gt;=0.2.3) (0.5.2)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: NumPy&gt;=1.17.0rc1 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from flaml[automl]) (1.26.2)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: lightgbm&gt;=2.3.1 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from flaml[automl]) (4.2.0)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: xgboost&gt;=0.90 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from flaml[automl]) (2.0.3)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: scipy&gt;=1.4.1 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from flaml[automl]) (1.11.4)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: pandas&gt;=1.1.4 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from flaml[automl]) (2.1.4)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: scikit-learn&gt;=0.24 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from flaml[automl]) (1.3.2)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: fastembed==0.1.1 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from qdrant_client[fastembed]) (0.1.1)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: grpcio&gt;=1.41.0 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from qdrant_client[fastembed]) (1.60.0)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: grpcio-tools&gt;=1.41.0 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from qdrant_client[fastembed]) (1.60.0)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: httpx&gt;=0.14.0 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from httpx[http2]&gt;=0.14.0-&gt;qdrant_client[fastembed]) (0.26.0)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: portalocker&lt;3.0.0,&gt;=2.7.0 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from qdrant_client[fastembed]) (2.8.2)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: urllib3&lt;2.0.0,&gt;=1.26.14 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from qdrant_client[fastembed]) (1.26.18)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: onnx&lt;2.0,&gt;=1.11 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from fastembed==0.1.1-&gt;qdrant_client[fastembed]) (1.15.0)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: onnxruntime&lt;2.0,&gt;=1.15 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from fastembed==0.1.1-&gt;qdrant_client[fastembed]) (1.16.3)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: requests&lt;3.0,&gt;=2.31 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from fastembed==0.1.1-&gt;qdrant_client[fastembed]) (2.31.0)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: tokenizers&lt;0.14,&gt;=0.13 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from fastembed==0.1.1-&gt;qdrant_client[fastembed]) (0.13.3)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: tqdm&lt;5.0,&gt;=4.65 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from fastembed==0.1.1-&gt;qdrant_client[fastembed]) (4.66.1)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: chromadb in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from pyautogen[retrievechat]&gt;=0.2.3) (0.4.21)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: ipython in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from pyautogen[retrievechat]&gt;=0.2.3) (8.19.0)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: pypdf in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from pyautogen[retrievechat]&gt;=0.2.3) (3.17.4)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: sentence-transformers in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from pyautogen[retrievechat]&gt;=0.2.3) (2.2.2)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: protobuf&lt;5.0dev,&gt;=4.21.6 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from grpcio-tools&gt;=1.41.0-&gt;qdrant_client[fastembed]) (4.25.1)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: setuptools in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from grpcio-tools&gt;=1.41.0-&gt;qdrant_client[fastembed]) (65.5.0)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: anyio in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from httpx&gt;=0.14.0-&gt;httpx[http2]&gt;=0.14.0-&gt;qdrant_client[fastembed]) (4.2.0)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: certifi in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from httpx&gt;=0.14.0-&gt;httpx[http2]&gt;=0.14.0-&gt;qdrant_client[fastembed]) (2023.11.17)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: httpcore==1.* in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from httpx&gt;=0.14.0-&gt;httpx[http2]&gt;=0.14.0-&gt;qdrant_client[fastembed]) (1.0.2)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: idna in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from httpx&gt;=0.14.0-&gt;httpx[http2]&gt;=0.14.0-&gt;qdrant_client[fastembed]) (3.6)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: sniffio in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from httpx&gt;=0.14.0-&gt;httpx[http2]&gt;=0.14.0-&gt;qdrant_client[fastembed]) (1.3.0)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: h11&lt;0.15,&gt;=0.13 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from httpcore==1.*-&gt;httpx&gt;=0.14.0-&gt;httpx[http2]&gt;=0.14.0-&gt;qdrant_client[fastembed]) (0.14.0)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: h2&lt;5,&gt;=3 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from httpx[http2]&gt;=0.14.0-&gt;qdrant_client[fastembed]) (4.1.0)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: distro&lt;2,&gt;=1.7.0 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from openai&gt;=1.3-&gt;pyautogen&gt;=0.2.3-&gt;pyautogen[retrievechat]&gt;=0.2.3) (1.9.0)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: typing-extensions&lt;5,&gt;=4.7 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from openai&gt;=1.3-&gt;pyautogen&gt;=0.2.3-&gt;pyautogen[retrievechat]&gt;=0.2.3) (4.9.0)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: python-dateutil&gt;=2.8.2 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from pandas&gt;=1.1.4-&gt;flaml[automl]) (2.8.2)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: pytz&gt;=2020.1 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from pandas&gt;=1.1.4-&gt;flaml[automl]) (2023.3.post1)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: tzdata&gt;=2022.1 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from pandas&gt;=1.1.4-&gt;flaml[automl]) (2023.4)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: annotated-types&gt;=0.4.0 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from pydantic&lt;3,&gt;=1.10-&gt;pyautogen&gt;=0.2.3-&gt;pyautogen[retrievechat]&gt;=0.2.3) (0.6.0)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: pydantic-core==2.14.6 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from pydantic&lt;3,&gt;=1.10-&gt;pyautogen&gt;=0.2.3-&gt;pyautogen[retrievechat]&gt;=0.2.3) (2.14.6)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: joblib&gt;=1.1.1 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from scikit-learn&gt;=0.24-&gt;flaml[automl]) (1.3.2)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: threadpoolctl&gt;=2.0.0 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from scikit-learn&gt;=0.24-&gt;flaml[automl]) (3.2.0)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: chroma-hnswlib==0.7.3 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from chromadb-&gt;pyautogen[retrievechat]&gt;=0.2.3) (0.7.3)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: fastapi&gt;=0.95.2 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from chromadb-&gt;pyautogen[retrievechat]&gt;=0.2.3) (0.108.0)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: uvicorn&gt;=0.18.3 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from uvicorn[standard]&gt;=0.18.3-&gt;chromadb-&gt;pyautogen[retrievechat]&gt;=0.2.3) (0.25.0)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: posthog&gt;=2.4.0 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from chromadb-&gt;pyautogen[retrievechat]&gt;=0.2.3) (3.1.0)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: pulsar-client&gt;=3.1.0 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from chromadb-&gt;pyautogen[retrievechat]&gt;=0.2.3) (3.3.0)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: opentelemetry-api&gt;=1.2.0 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from chromadb-&gt;pyautogen[retrievechat]&gt;=0.2.3) (1.22.0)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc&gt;=1.2.0 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from chromadb-&gt;pyautogen[retrievechat]&gt;=0.2.3) (1.22.0)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: opentelemetry-instrumentation-fastapi&gt;=0.41b0 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from chromadb-&gt;pyautogen[retrievechat]&gt;=0.2.3) (0.43b0)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: opentelemetry-sdk&gt;=1.2.0 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from chromadb-&gt;pyautogen[retrievechat]&gt;=0.2.3) (1.22.0)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: pypika&gt;=0.48.9 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from chromadb-&gt;pyautogen[retrievechat]&gt;=0.2.3) (0.48.9)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: overrides&gt;=7.3.1 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from chromadb-&gt;pyautogen[retrievechat]&gt;=0.2.3) (7.4.0)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: importlib-resources in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from chromadb-&gt;pyautogen[retrievechat]&gt;=0.2.3) (6.1.1)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: bcrypt&gt;=4.0.1 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from chromadb-&gt;pyautogen[retrievechat]&gt;=0.2.3) (4.1.2)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: typer&gt;=0.9.0 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from chromadb-&gt;pyautogen[retrievechat]&gt;=0.2.3) (0.9.0)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: kubernetes&gt;=28.1.0 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from chromadb-&gt;pyautogen[retrievechat]&gt;=0.2.3) (28.1.0)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: tenacity&gt;=8.2.3 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from chromadb-&gt;pyautogen[retrievechat]&gt;=0.2.3) (8.2.3)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: PyYAML&gt;=6.0.0 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from chromadb-&gt;pyautogen[retrievechat]&gt;=0.2.3) (6.0.1)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: mmh3&gt;=4.0.1 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from chromadb-&gt;pyautogen[retrievechat]&gt;=0.2.3) (4.0.1)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: decorator in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from ipython-&gt;pyautogen[retrievechat]&gt;=0.2.3) (5.1.1)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: jedi&gt;=0.16 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from ipython-&gt;pyautogen[retrievechat]&gt;=0.2.3) (0.19.1)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: matplotlib-inline in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from ipython-&gt;pyautogen[retrievechat]&gt;=0.2.3) (0.1.6)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: prompt-toolkit&lt;3.1.0,&gt;=3.0.41 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from ipython-&gt;pyautogen[retrievechat]&gt;=0.2.3) (3.0.43)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: pygments&gt;=2.4.0 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from ipython-&gt;pyautogen[retrievechat]&gt;=0.2.3) (2.17.2)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: stack-data in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from ipython-&gt;pyautogen[retrievechat]&gt;=0.2.3) (0.6.3)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: traitlets&gt;=5 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from ipython-&gt;pyautogen[retrievechat]&gt;=0.2.3) (5.14.1)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: pexpect&gt;4.3 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from ipython-&gt;pyautogen[retrievechat]&gt;=0.2.3) (4.9.0)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: transformers&lt;5.0.0,&gt;=4.6.0 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from sentence-transformers-&gt;pyautogen[retrievechat]&gt;=0.2.3) (4.33.3)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: torch&gt;=1.6.0 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from sentence-transformers-&gt;pyautogen[retrievechat]&gt;=0.2.3) (2.1.2)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: torchvision in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from sentence-transformers-&gt;pyautogen[retrievechat]&gt;=0.2.3) (0.16.2)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: nltk in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from sentence-transformers-&gt;pyautogen[retrievechat]&gt;=0.2.3) (3.8.1)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: sentencepiece in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from sentence-transformers-&gt;pyautogen[retrievechat]&gt;=0.2.3) (0.1.99)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: huggingface-hub&gt;=0.4.0 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from sentence-transformers-&gt;pyautogen[retrievechat]&gt;=0.2.3) (0.20.1)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: regex&gt;=2022.1.18 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from tiktoken-&gt;pyautogen&gt;=0.2.3-&gt;pyautogen[retrievechat]&gt;=0.2.3) (2023.12.25)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: starlette&lt;0.33.0,&gt;=0.29.0 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from fastapi&gt;=0.95.2-&gt;chromadb-&gt;pyautogen[retrievechat]&gt;=0.2.3) (0.32.0.post1)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: hyperframe&lt;7,&gt;=6.0 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from h2&lt;5,&gt;=3-&gt;httpx[http2]&gt;=0.14.0-&gt;qdrant_client[fastembed]) (6.0.1)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: hpack&lt;5,&gt;=4.0 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from h2&lt;5,&gt;=3-&gt;httpx[http2]&gt;=0.14.0-&gt;qdrant_client[fastembed]) (4.0.0)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: filelock in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from huggingface-hub&gt;=0.4.0-&gt;sentence-transformers-&gt;pyautogen[retrievechat]&gt;=0.2.3) (3.13.1)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: fsspec&gt;=2023.5.0 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from huggingface-hub&gt;=0.4.0-&gt;sentence-transformers-&gt;pyautogen[retrievechat]&gt;=0.2.3) (2023.12.2)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: packaging&gt;=20.9 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from huggingface-hub&gt;=0.4.0-&gt;sentence-transformers-&gt;pyautogen[retrievechat]&gt;=0.2.3) (23.2)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: parso&lt;0.9.0,&gt;=0.8.3 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from jedi&gt;=0.16-&gt;ipython-&gt;pyautogen[retrievechat]&gt;=0.2.3) (0.8.3)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: six&gt;=1.9.0 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from kubernetes&gt;=28.1.0-&gt;chromadb-&gt;pyautogen[retrievechat]&gt;=0.2.3) (1.16.0)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: google-auth&gt;=1.0.1 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from kubernetes&gt;=28.1.0-&gt;chromadb-&gt;pyautogen[retrievechat]&gt;=0.2.3) (2.25.2)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,&gt;=0.32.0 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from kubernetes&gt;=28.1.0-&gt;chromadb-&gt;pyautogen[retrievechat]&gt;=0.2.3) (1.7.0)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: requests-oauthlib in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from kubernetes&gt;=28.1.0-&gt;chromadb-&gt;pyautogen[retrievechat]&gt;=0.2.3) (1.3.1)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: oauthlib&gt;=3.2.2 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from kubernetes&gt;=28.1.0-&gt;chromadb-&gt;pyautogen[retrievechat]&gt;=0.2.3) (3.2.2)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: coloredlogs in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from onnxruntime&lt;2.0,&gt;=1.15-&gt;fastembed==0.1.1-&gt;qdrant_client[fastembed]) (15.0.1)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: flatbuffers in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from onnxruntime&lt;2.0,&gt;=1.15-&gt;fastembed==0.1.1-&gt;qdrant_client[fastembed]) (23.5.26)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: sympy in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from onnxruntime&lt;2.0,&gt;=1.15-&gt;fastembed==0.1.1-&gt;qdrant_client[fastembed]) (1.12)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: deprecated&gt;=1.2.6 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from opentelemetry-api&gt;=1.2.0-&gt;chromadb-&gt;pyautogen[retrievechat]&gt;=0.2.3) (1.2.14)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: importlib-metadata&lt;7.0,&gt;=6.0 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from opentelemetry-api&gt;=1.2.0-&gt;chromadb-&gt;pyautogen[retrievechat]&gt;=0.2.3) (6.11.0)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: backoff&lt;3.0.0,&gt;=1.10.0 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc&gt;=1.2.0-&gt;chromadb-&gt;pyautogen[retrievechat]&gt;=0.2.3) (2.2.1)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: googleapis-common-protos~=1.52 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc&gt;=1.2.0-&gt;chromadb-&gt;pyautogen[retrievechat]&gt;=0.2.3) (1.62.0)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.22.0 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc&gt;=1.2.0-&gt;chromadb-&gt;pyautogen[retrievechat]&gt;=0.2.3) (1.22.0)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: opentelemetry-proto==1.22.0 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc&gt;=1.2.0-&gt;chromadb-&gt;pyautogen[retrievechat]&gt;=0.2.3) (1.22.0)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: opentelemetry-instrumentation-asgi==0.43b0 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi&gt;=0.41b0-&gt;chromadb-&gt;pyautogen[retrievechat]&gt;=0.2.3) (0.43b0)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: opentelemetry-instrumentation==0.43b0 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi&gt;=0.41b0-&gt;chromadb-&gt;pyautogen[retrievechat]&gt;=0.2.3) (0.43b0)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: opentelemetry-semantic-conventions==0.43b0 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi&gt;=0.41b0-&gt;chromadb-&gt;pyautogen[retrievechat]&gt;=0.2.3) (0.43b0)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: opentelemetry-util-http==0.43b0 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi&gt;=0.41b0-&gt;chromadb-&gt;pyautogen[retrievechat]&gt;=0.2.3) (0.43b0)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: wrapt&lt;2.0.0,&gt;=1.0.0 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from opentelemetry-instrumentation==0.43b0-&gt;opentelemetry-instrumentation-fastapi&gt;=0.41b0-&gt;chromadb-&gt;pyautogen[retrievechat]&gt;=0.2.3) (1.16.0)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: asgiref~=3.0 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from opentelemetry-instrumentation-asgi==0.43b0-&gt;opentelemetry-instrumentation-fastapi&gt;=0.41b0-&gt;chromadb-&gt;pyautogen[retrievechat]&gt;=0.2.3) (3.7.2)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: ptyprocess&gt;=0.5 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from pexpect&gt;4.3-&gt;ipython-&gt;pyautogen[retrievechat]&gt;=0.2.3) (0.7.0)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: monotonic&gt;=1.5 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from posthog&gt;=2.4.0-&gt;chromadb-&gt;pyautogen[retrievechat]&gt;=0.2.3) (1.6)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: wcwidth in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from prompt-toolkit&lt;3.1.0,&gt;=3.0.41-&gt;ipython-&gt;pyautogen[retrievechat]&gt;=0.2.3) (0.2.12)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from requests&lt;3.0,&gt;=2.31-&gt;fastembed==0.1.1-&gt;qdrant_client[fastembed]) (3.3.2)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: networkx in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from torch&gt;=1.6.0-&gt;sentence-transformers-&gt;pyautogen[retrievechat]&gt;=0.2.3) (3.2.1)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: jinja2 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from torch&gt;=1.6.0-&gt;sentence-transformers-&gt;pyautogen[retrievechat]&gt;=0.2.3) (3.1.2)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from torch&gt;=1.6.0-&gt;sentence-transformers-&gt;pyautogen[retrievechat]&gt;=0.2.3) (12.1.105)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from torch&gt;=1.6.0-&gt;sentence-transformers-&gt;pyautogen[retrievechat]&gt;=0.2.3) (12.1.105)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from torch&gt;=1.6.0-&gt;sentence-transformers-&gt;pyautogen[retrievechat]&gt;=0.2.3) (12.1.105)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from torch&gt;=1.6.0-&gt;sentence-transformers-&gt;pyautogen[retrievechat]&gt;=0.2.3) (8.9.2.26)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from torch&gt;=1.6.0-&gt;sentence-transformers-&gt;pyautogen[retrievechat]&gt;=0.2.3) (12.1.3.1)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from torch&gt;=1.6.0-&gt;sentence-transformers-&gt;pyautogen[retrievechat]&gt;=0.2.3) (11.0.2.54)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from torch&gt;=1.6.0-&gt;sentence-transformers-&gt;pyautogen[retrievechat]&gt;=0.2.3) (10.3.2.106)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from torch&gt;=1.6.0-&gt;sentence-transformers-&gt;pyautogen[retrievechat]&gt;=0.2.3) (11.4.5.107)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from torch&gt;=1.6.0-&gt;sentence-transformers-&gt;pyautogen[retrievechat]&gt;=0.2.3) (12.1.0.106)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from torch&gt;=1.6.0-&gt;sentence-transformers-&gt;pyautogen[retrievechat]&gt;=0.2.3) (2.18.1)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from torch&gt;=1.6.0-&gt;sentence-transformers-&gt;pyautogen[retrievechat]&gt;=0.2.3) (12.1.105)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: triton==2.1.0 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from torch&gt;=1.6.0-&gt;sentence-transformers-&gt;pyautogen[retrievechat]&gt;=0.2.3) (2.1.0)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: nvidia-nvjitlink-cu12 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107-&gt;torch&gt;=1.6.0-&gt;sentence-transformers-&gt;pyautogen[retrievechat]&gt;=0.2.3) (12.3.101)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: safetensors&gt;=0.3.1 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from transformers&lt;5.0.0,&gt;=4.6.0-&gt;sentence-transformers-&gt;pyautogen[retrievechat]&gt;=0.2.3) (0.4.1)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: click&lt;9.0.0,&gt;=7.1.1 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from typer&gt;=0.9.0-&gt;chromadb-&gt;pyautogen[retrievechat]&gt;=0.2.3) (8.1.7)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: httptools&gt;=0.5.0 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from uvicorn[standard]&gt;=0.18.3-&gt;chromadb-&gt;pyautogen[retrievechat]&gt;=0.2.3) (0.6.1)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,&gt;=0.14.0 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from uvicorn[standard]&gt;=0.18.3-&gt;chromadb-&gt;pyautogen[retrievechat]&gt;=0.2.3) (0.19.0)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: watchfiles&gt;=0.13 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from uvicorn[standard]&gt;=0.18.3-&gt;chromadb-&gt;pyautogen[retrievechat]&gt;=0.2.3) (0.21.0)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: websockets&gt;=10.4 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from uvicorn[standard]&gt;=0.18.3-&gt;chromadb-&gt;pyautogen[retrievechat]&gt;=0.2.3) (12.0)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: executing&gt;=1.2.0 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from stack-data-&gt;ipython-&gt;pyautogen[retrievechat]&gt;=0.2.3) (2.0.1)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: asttokens&gt;=2.1.0 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from stack-data-&gt;ipython-&gt;pyautogen[retrievechat]&gt;=0.2.3) (2.4.1)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: pure-eval in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from stack-data-&gt;ipython-&gt;pyautogen[retrievechat]&gt;=0.2.3) (0.2.2)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: pillow!=8.3.*,&gt;=5.3.0 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from torchvision-&gt;sentence-transformers-&gt;pyautogen[retrievechat]&gt;=0.2.3) (10.2.0)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: cachetools&lt;6.0,&gt;=2.0.0 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from google-auth&gt;=1.0.1-&gt;kubernetes&gt;=28.1.0-&gt;chromadb-&gt;pyautogen[retrievechat]&gt;=0.2.3) (5.3.2)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: pyasn1-modules&gt;=0.2.1 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from google-auth&gt;=1.0.1-&gt;kubernetes&gt;=28.1.0-&gt;chromadb-&gt;pyautogen[retrievechat]&gt;=0.2.3) (0.3.0)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: rsa&lt;5,&gt;=3.1.4 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from google-auth&gt;=1.0.1-&gt;kubernetes&gt;=28.1.0-&gt;chromadb-&gt;pyautogen[retrievechat]&gt;=0.2.3) (4.9)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: zipp&gt;=0.5 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from importlib-metadata&lt;7.0,&gt;=6.0-&gt;opentelemetry-api&gt;=1.2.0-&gt;chromadb-&gt;pyautogen[retrievechat]&gt;=0.2.3) (3.17.0)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: humanfriendly&gt;=9.1 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from coloredlogs-&gt;onnxruntime&lt;2.0,&gt;=1.15-&gt;fastembed==0.1.1-&gt;qdrant_client[fastembed]) (10.0)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: MarkupSafe&gt;=2.0 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from jinja2-&gt;torch&gt;=1.6.0-&gt;sentence-transformers-&gt;pyautogen[retrievechat]&gt;=0.2.3) (2.1.3)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: mpmath&gt;=0.19 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from sympy-&gt;onnxruntime&lt;2.0,&gt;=1.15-&gt;fastembed==0.1.1-&gt;qdrant_client[fastembed]) (1.3.0)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Requirement already satisfied: pyasn1&lt;0.6.0,&gt;=0.4.6 in /workspaces/autogen/.venv-3.11/lib/python3.11/site-packages (from pyasn1-modules&gt;=0.2.1-&gt;google-auth&gt;=1.0.1-&gt;kubernetes&gt;=28.1.0-&gt;chromadb-&gt;pyautogen[retrievechat]&gt;=0.2.3) (0.5.1)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Note: you may need to restart the kernel to use updated packages.</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="set-your-api-endpoint">Set your API Endpoint<a href="#set-your-api-endpoint" class="hash-link" aria-label="Direct link to Set your API Endpoint" title="Direct link to Set your API Endpoint">​</a></h2>
<p>The
<a href="https://microsoft.github.io/autogen/docs/reference/oai/openai_utils#config_list_from_json" target="_blank" rel="noopener noreferrer"><code>config_list_from_json</code></a>
function loads a list of configurations from an environment variable or
a json file.</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token keyword" style="font-style:italic">from</span><span class="token plain"> qdrant_client </span><span class="token keyword" style="font-style:italic">import</span><span class="token plain"> QdrantClient</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token keyword" style="font-style:italic">import</span><span class="token plain"> autogen</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token keyword" style="font-style:italic">from</span><span class="token plain"> autogen</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">agentchat</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">contrib</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">qdrant_retrieve_user_proxy_agent </span><span class="token keyword" style="font-style:italic">import</span><span class="token plain"> QdrantRetrieveUserProxyAgent</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token keyword" style="font-style:italic">from</span><span class="token plain"> autogen</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">agentchat</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">contrib</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">retrieve_assistant_agent </span><span class="token keyword" style="font-style:italic">import</span><span class="token plain"> RetrieveAssistantAgent</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic"># Accepted file formats for that can be stored in</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic"># a vector database instance</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token keyword" style="font-style:italic">from</span><span class="token plain"> autogen</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">retrieve_utils </span><span class="token keyword" style="font-style:italic">import</span><span class="token plain"> TEXT_FORMATS</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">config_list </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> autogen</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">config_list_from_json</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    env_or_file</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token string" style="color:rgb(195, 232, 141)">&quot;OAI_CONFIG_LIST&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    file_location</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token string" style="color:rgb(195, 232, 141)">&quot;.&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    filter_dict</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token punctuation" style="color:rgb(199, 146, 234)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">        </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;model&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(199, 146, 234)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">            </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;gpt-4&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">            </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;gpt4&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">            </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;gpt-4-32k&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">            </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;gpt-4-32k-0314&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">            </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;gpt-35-turbo&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">            </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;gpt-3.5-turbo&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">        </span><span class="token punctuation" style="color:rgb(199, 146, 234)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(199, 146, 234)">}</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token keyword" style="font-style:italic">assert</span><span class="token plain"> </span><span class="token builtin" style="color:rgb(130, 170, 255)">len</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">config_list</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"> </span><span class="token operator" style="color:rgb(137, 221, 255)">&gt;</span><span class="token plain"> </span><span class="token number" style="color:rgb(247, 140, 108)">0</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token keyword" style="font-style:italic">print</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token string" style="color:rgb(195, 232, 141)">&quot;models to use: &quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(199, 146, 234)">[</span><span class="token plain">config_list</span><span class="token punctuation" style="color:rgb(199, 146, 234)">[</span><span class="token plain">i</span><span class="token punctuation" style="color:rgb(199, 146, 234)">]</span><span class="token punctuation" style="color:rgb(199, 146, 234)">[</span><span class="token string" style="color:rgb(195, 232, 141)">&quot;model&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">]</span><span class="token plain"> </span><span class="token keyword" style="font-style:italic">for</span><span class="token plain"> i </span><span class="token keyword" style="font-style:italic">in</span><span class="token plain"> </span><span class="token builtin" style="color:rgb(130, 170, 255)">range</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token builtin" style="color:rgb(130, 170, 255)">len</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">config_list</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token punctuation" style="color:rgb(199, 146, 234)">]</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token plain">models to use:  [&#x27;gpt-4-1106-preview&#x27;, &#x27;gpt-4-turbo-preview&#x27;, &#x27;gpt-4-0613&#x27;, &#x27;gpt-35-turbo-0613&#x27;, &#x27;gpt-35-turbo-1106&#x27;]</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_BuS1"><p>Learn more about configuring LLMs for agents <a href="/autogen/docs/topics/llm_configuration">here</a>.</p></div></div>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token keyword" style="font-style:italic">print</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token string" style="color:rgb(195, 232, 141)">&quot;Accepted file formats for `docs_path`:&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token keyword" style="font-style:italic">print</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">TEXT_FORMATS</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token plain">Accepted file formats for `docs_path`:</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">[&#x27;txt&#x27;, &#x27;json&#x27;, &#x27;csv&#x27;, &#x27;tsv&#x27;, &#x27;md&#x27;, &#x27;html&#x27;, &#x27;htm&#x27;, &#x27;rtf&#x27;, &#x27;rst&#x27;, &#x27;jsonl&#x27;, &#x27;log&#x27;, &#x27;xml&#x27;, &#x27;yaml&#x27;, &#x27;yml&#x27;, &#x27;pdf&#x27;]</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="construct-agents-for-retrievechat">Construct agents for RetrieveChat<a href="#construct-agents-for-retrievechat" class="hash-link" aria-label="Direct link to Construct agents for RetrieveChat" title="Direct link to Construct agents for RetrieveChat">​</a></h2>
<p>We start by initializing the <code>RetrieveAssistantAgent</code> and
<code>QdrantRetrieveUserProxyAgent</code>. The system message needs to be set to
“You are a helpful assistant.” for RetrieveAssistantAgent. The detailed
instructions are given in the user message. Later we will use the
<code>QdrantRetrieveUserProxyAgent.generate_init_prompt</code> to combine the
instructions and a retrieval augmented generation task for an initial
prompt to be sent to the LLM assistant.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="you-can-find-the-list-of-all-the-embedding-models-supported-by-qdrant-here.">You can find the list of all the embedding models supported by Qdrant <a href="https://qdrant.github.io/fastembed/examples/Supported_Models/" target="_blank" rel="noopener noreferrer">here</a>.<a href="#you-can-find-the-list-of-all-the-embedding-models-supported-by-qdrant-here." class="hash-link" aria-label="Direct link to you-can-find-the-list-of-all-the-embedding-models-supported-by-qdrant-here." title="Direct link to you-can-find-the-list-of-all-the-embedding-models-supported-by-qdrant-here.">​</a></h3>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic"># 1. create an RetrieveAssistantAgent instance named &quot;assistant&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">assistant </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> RetrieveAssistantAgent</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    name</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token string" style="color:rgb(195, 232, 141)">&quot;assistant&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    system_message</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token string" style="color:rgb(195, 232, 141)">&quot;You are a helpful assistant.&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    llm_config</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token punctuation" style="color:rgb(199, 146, 234)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">        </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;timeout&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token number" style="color:rgb(247, 140, 108)">600</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">        </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;cache_seed&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token number" style="color:rgb(247, 140, 108)">42</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">        </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;config_list&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> config_list</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(199, 146, 234)">}</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic"># 2. create the QdrantRetrieveUserProxyAgent instance named &quot;ragproxyagent&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic"># By default, the human_input_mode is &quot;ALWAYS&quot;, which means the agent will ask for human input at every step. We set it to &quot;NEVER&quot; here.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic"># `docs_path` is the path to the docs directory. It can also be the path to a single file, or the url to a single file. By default,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic"># it is set to None, which works only if the collection is already created.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic">#</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic"># Here we generated the documentations from FLAML&#x27;s docstrings. Not needed if you just want to try this notebook but not to reproduce the</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic"># outputs. Clone the FLAML (https://github.com/microsoft/FLAML) repo and navigate to its website folder. Pip install and run `pydoc-markdown`</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic"># and it will generate folder `reference` under `website/docs`.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic">#</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic"># `task` indicates the kind of task we&#x27;re working on. In this example, it&#x27;s a `code` task.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic"># `chunk_token_size` is the chunk token size for the retrieve chat. By default, it is set to `max_tokens * 0.6`, here we set it to 2000.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic"># We use an in-memory QdrantClient instance here. Not recommended for production.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic"># Get the installation instructions here: https://qdrant.tech/documentation/guides/installation/</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">ragproxyagent </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> QdrantRetrieveUserProxyAgent</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    name</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token string" style="color:rgb(195, 232, 141)">&quot;ragproxyagent&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    human_input_mode</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token string" style="color:rgb(195, 232, 141)">&quot;NEVER&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    max_consecutive_auto_reply</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token number" style="color:rgb(247, 140, 108)">10</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    retrieve_config</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token punctuation" style="color:rgb(199, 146, 234)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">        </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;task&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;code&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">        </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;docs_path&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(199, 146, 234)">[</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">            </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;https://raw.githubusercontent.com/microsoft/flaml/main/README.md&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">            </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;https://raw.githubusercontent.com/microsoft/FLAML/main/website/docs/Research.md&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">        </span><span class="token punctuation" style="color:rgb(199, 146, 234)">]</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain">  </span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic"># change this to your own path, such as https://raw.githubusercontent.com/microsoft/autogen/main/README.md</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">        </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;chunk_token_size&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token number" style="color:rgb(247, 140, 108)">2000</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">        </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;model&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> config_list</span><span class="token punctuation" style="color:rgb(199, 146, 234)">[</span><span class="token number" style="color:rgb(247, 140, 108)">0</span><span class="token punctuation" style="color:rgb(199, 146, 234)">]</span><span class="token punctuation" style="color:rgb(199, 146, 234)">[</span><span class="token string" style="color:rgb(195, 232, 141)">&quot;model&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">]</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">        </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;client&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> QdrantClient</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token string" style="color:rgb(195, 232, 141)">&quot;:memory:&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">        </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;embedding_model&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;BAAI/bge-small-en-v1.5&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(199, 146, 234)">}</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic"># code_execution_config={</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic">#     &quot;use_docker&quot;: False,}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p><a id="example-1"></a> ### Example 1</p>
<p><a href="#toc">back to top</a></p>
<p>Use RetrieveChat to answer a question and ask for human-in-loop
feedbacks.</p>
<p>Problem: Is there a function named <code>tune_automl</code> in FLAML?</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic"># reset the assistant. Always reset the assistant before starting a new conversation.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">assistant</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">reset</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">qa_problem </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;Is there a function called tune_automl?&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">ragproxyagent</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">initiate_chat</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">assistant</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> message</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">ragproxyagent</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">message_generator</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> problem</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">qa_problem</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token plain">Trying to create collection.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Adding doc_id 0 to context.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Adding doc_id 2 to context.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Adding doc_id 1 to context.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token message-id class-name" style="color:rgb(255, 203, 107)">ragproxyagent</span><span class="token message-id"> </span><span class="token message-id punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token message-id keyword" style="font-style:italic">to</span><span class="token message-id"> </span><span class="token message-id class-name" style="color:rgb(255, 203, 107)">assistant</span><span class="token message-id punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token message-id">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">You&#x27;re a retrieve augmented coding assistant. You answer user&#x27;s questions based on your own knowledge and the</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">context provided by the user.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">If you can&#x27;t answer the question with or without the current context, you should reply exactly `UPDATE CONTEXT`.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">For code generation, you must obey the following rules:</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Rule 1. You MUST NOT install any packages because all the packages needed are already installed.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Rule 2. You must follow the formats below to write your code:</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```language</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"># your code</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">User&#x27;s question is: Is there a function called tune_automl?</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Context is: [![PyPI version](https://badge.fury.io/py/FLAML.svg)](https://badge.fury.io/py/FLAML)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">![Conda version](https://img.shields.io/conda/vn/conda-forge/flaml)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">[![Build](https://github.com/microsoft/FLAML/actions/workflows/python-package.yml/badge.svg)](https://github.com/microsoft/FLAML/actions/workflows/python-package.yml)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">![Python Version](https://img.shields.io/badge/3.8%20%7C%203.9%20%7C%203.10-blue)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">[![Downloads](https://pepy.tech/badge/flaml)](https://pepy.tech/project/flaml)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">[![](https://img.shields.io/discord/1025786666260111483?logo=discord&amp;style=flat)](https://discord.gg/Cppx2vSPVP)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">&lt;!-- [![Join the chat at https://gitter.im/FLAMLer/community](https://badges.gitter.im/FLAMLer/community.svg)](https://gitter.im/FLAMLer/community?utm_source=badge&amp;utm_medium=badge&amp;utm_campaign=pr-badge&amp;utm_content=badge) --&gt;</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"># A Fast Library for Automated Machine Learning &amp; Tuning</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">&lt;p align=&quot;center&quot;&gt;</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    &lt;img src=&quot;https://github.com/microsoft/FLAML/blob/main/website/static/img/flaml.svg&quot;  width=200&gt;</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    &lt;br&gt;</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">&lt;/p&gt;</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">:fire: Heads-up: We have migrated [AutoGen](https://microsoft.github.io/autogen/) into a dedicated [github repository](https://github.com/microsoft/autogen). Alongside this move, we have also launched a dedicated [Discord](https://discord.gg/pAbnFJrkgZ) server and a [website](https://microsoft.github.io/autogen/) for comprehensive documentation.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">:fire: The automated multi-agent chat framework in [AutoGen](https://microsoft.github.io/autogen/) is in preview from v2.0.0.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">:fire: FLAML is highlighted in OpenAI&#x27;s [cookbook](https://github.com/openai/openai-cookbook#related-resources-from-around-the-web).</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">:fire: [autogen](https://microsoft.github.io/autogen/) is released with support for ChatGPT and GPT-4, based on [Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference](https://arxiv.org/abs/2303.04673).</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">:fire: FLAML supports Code-First AutoML &amp; Tuning – Private Preview in [Microsoft Fabric Data Science](https://learn.microsoft.com/en-us/fabric/data-science/).</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">## What is FLAML</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">FLAML is a lightweight Python library for efficient automation of machine</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">learning and AI operations. It automates workflow based on large language models, machine learning models, etc.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">and optimizes their performance.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">* FLAML enables building next-gen GPT-X applications based on multi-agent conversations with minimal effort. It simplifies the orchestration, automation and optimization of a complex GPT-X workflow. It maximizes the performance of GPT-X models and augments their weakness.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">* For common machine learning tasks like classification and regression, it quickly finds quality models for user-provided data with low computational resources. It is easy to customize or extend. Users can find their desired customizability from a smooth range.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">* It supports fast and economical automatic tuning (e.g., inference hyperparameters for foundation models, configurations in MLOps/LMOps workflows, pipelines, mathematical/statistical models, algorithms, computing experiments, software configurations), capable of handling large search space with heterogeneous evaluation cost and complex constraints/guidance/early stopping.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">FLAML is powered by a series of [research studies](https://microsoft.github.io/FLAML/docs/Research/) from Microsoft Research and collaborators such as Penn State University, Stevens Institute of Technology, University of Washington, and University of Waterloo.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">FLAML has a .NET implementation in [ML.NET](http://dot.net/ml), an open-source, cross-platform machine learning framework for .NET.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">## Installation</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">FLAML requires **Python version &gt;= 3.8**. It can be installed from pip:</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```bash</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">pip install flaml</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Minimal dependencies are installed without extra options. You can install extra options based on the feature you need. For example, use the following to install the dependencies needed by the [`autogen`](https://microsoft.github.io/autogen/) package.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```bash</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">pip install &quot;flaml[autogen]&quot;</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Find more options in [Installation](https://microsoft.github.io/FLAML/docs/Installation).</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Each of the [`notebook examples`](https://github.com/microsoft/FLAML/tree/main/notebook) may require a specific option to be installed.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">## Quickstart</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">* (New) The [autogen](https://microsoft.github.io/autogen/) package enables the next-gen GPT-X applications with a generic multi-agent conversation framework.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">It offers customizable and conversable agents which integrate LLMs, tools and human.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">By automating chat among multiple capable agents, one can easily make them collectively perform tasks autonomously or with human feedback, including tasks that require using tools via code. For example,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```python</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">from flaml import autogen</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">assistant = autogen.AssistantAgent(&quot;assistant&quot;)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">user_proxy = autogen.UserProxyAgent(&quot;user_proxy&quot;)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">user_proxy.initiate_chat(assistant, message=&quot;Show me the YTD gain of 10 largest technology companies as of today.&quot;)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"># This initiates an automated chat between the two agents to solve the task</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Autogen also helps maximize the utility out of the expensive LLMs such as ChatGPT and GPT-4. It offers a drop-in replacement of `openai.Completion` or `openai.ChatCompletion` with powerful functionalites like tuning, caching, templating, filtering. For example, you can optimize generations by LLM with your own tuning data, success metrics and budgets.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```python</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"># perform tuning</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">config, analysis = autogen.Completion.tune(</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    data=tune_data,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    metric=&quot;success&quot;,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    mode=&quot;max&quot;,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    eval_func=eval_func,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    inference_budget=0.05,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    optimization_budget=3,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    num_samples=-1,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"># perform inference for a test instance</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">response = autogen.Completion.create(context=test_instance, **config)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">* With three lines of code, you can start using this economical and fast</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">AutoML engine as a [scikit-learn style estimator](https://microsoft.github.io/FLAML/docs/Use-Cases/Task-Oriented-AutoML).</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```python</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">from flaml import AutoML</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">automl = AutoML()</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">automl.fit(X_train, y_train, task=&quot;classification&quot;)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">* You can restrict the learners and use FLAML as a fast hyperparameter tuning</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">tool for XGBoost, LightGBM, Random Forest etc. or a [customized learner](https://microsoft.github.io/FLAML/docs/Use-Cases/Task-Oriented-AutoML#estimator-and-search-space).</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```python</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">automl.fit(X_train, y_train, task=&quot;classification&quot;, estimator_list=[&quot;lgbm&quot;])</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">* You can also run generic hyperparameter tuning for a [custom function](https://microsoft.github.io/FLAML/docs/Use-Cases/Tune-User-Defined-Function).</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```python</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">from flaml import tune</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">tune.run(evaluation_function, config={…}, low_cost_partial_config={…}, time_budget_s=3600)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">* [Zero-shot AutoML](https://microsoft.github.io/FLAML/docs/Use-Cases/Zero-Shot-AutoML) allows using the existing training API from lightgbm, xgboost etc. while getting the benefit of AutoML in choosing high-performance hyperparameter configurations per task.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```python</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">from flaml.default import LGBMRegressor</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"># Use LGBMRegressor in the same way as you use lightgbm.LGBMRegressor.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">estimator = LGBMRegressor()</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"># The hyperparameters are automatically set according to the training data.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">estimator.fit(X_train, y_train)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">## Documentation</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">You can find a detailed documentation about FLAML [here](https://microsoft.github.io/FLAML/).</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">In addition, you can find:</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">- [Research](https://microsoft.github.io/FLAML/docs/Research) and [blogposts](https://microsoft.github.io/FLAML/blog) around FLAML.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">- [Discord](https://discord.gg/Cppx2vSPVP).</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">- [Contributing guide](https://microsoft.github.io/FLAML/docs/Contribute).</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">- ML.NET documentation and tutorials for [Model Builder](https://learn.microsoft.com/dotnet/machine-learning/tutorials/predict-prices-with-model-builder), [ML.NET CLI](https://learn.microsoft.com/dotnet/machine-learning/tutorials/sentiment-analysis-cli), and [AutoML API](https://learn.microsoft.com/dotnet/machine-learning/how-to-guides/how-to-use-the-automl-api).</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">## Contributing</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">This project welcomes contributions and suggestions. Most contributions require you to agree to a</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">the rights to use your contribution. For details, visit &lt;https://cla.opensource.microsoft.com&gt;.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">If you are new to GitHub [here](https://help.github.com/categories/collaborating-with-issues-and-pull-requests/) is a detailed help source on getting involved with development on GitHub.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"># Research</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">For technical details, please check our research publications.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">* [FLAML: A Fast and Lightweight AutoML Library](https://www.microsoft.com/en-us/research/publication/flaml-a-fast-and-lightweight-automl-library/). Chi Wang, Qingyun Wu, Markus Weimer, Erkang Zhu. MLSys 2021.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```bibtex</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">@inproceedings{wang2021flaml,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    title={FLAML: A Fast and Lightweight AutoML Library},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    author={Chi Wang and Qingyun Wu and Markus Weimer and Erkang Zhu},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    year={2021},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    booktitle={MLSys},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">}</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">* [Frugal Optimization for Cost-related Hyperparameters](https://arxiv.org/abs/2005.01571). Qingyun Wu, Chi Wang, Silu Huang. AAAI 2021.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```bibtex</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">@inproceedings{wu2021cfo,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    title={Frugal Optimization for Cost-related Hyperparameters},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    author={Qingyun Wu and Chi Wang and Silu Huang},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    year={2021},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    booktitle={AAAI},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">}</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">* [Economical Hyperparameter Optimization With Blended Search Strategy](https://www.microsoft.com/en-us/research/publication/economical-hyperparameter-optimization-with-blended-search-strategy/). Chi Wang, Qingyun Wu, Silu Huang, Amin Saied. ICLR 2021.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```bibtex</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">@inproceedings{wang2021blendsearch,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    title={Economical Hyperparameter Optimization With Blended Search Strategy},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    author={Chi Wang and Qingyun Wu and Silu Huang and Amin Saied},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    year={2021},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    booktitle={ICLR},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">}</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">* [An Empirical Study on Hyperparameter Optimization for Fine-Tuning Pre-trained Language Models](https://aclanthology.org/2021.acl-long.178.pdf). Susan Xueqing Liu, Chi Wang. ACL 2021.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```bibtex</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">@inproceedings{liuwang2021hpolm,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    title={An Empirical Study on Hyperparameter Optimization for Fine-Tuning Pre-trained Language Models},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    author={Susan Xueqing Liu and Chi Wang},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    year={2021},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    booktitle={ACL},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">}</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">* [ChaCha for Online AutoML](https://www.microsoft.com/en-us/research/publication/chacha-for-online-automl/). Qingyun Wu, Chi Wang, John Langford, Paul Mineiro and Marco Rossi. ICML 2021.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```bibtex</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">@inproceedings{wu2021chacha,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    title={ChaCha for Online AutoML},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    author={Qingyun Wu and Chi Wang and John Langford and Paul Mineiro and Marco Rossi},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    year={2021},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    booktitle={ICML},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">}</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">* [Fair AutoML](https://arxiv.org/abs/2111.06495). Qingyun Wu, Chi Wang. ArXiv preprint arXiv:2111.06495 (2021).</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```bibtex</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">@inproceedings{wuwang2021fairautoml,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    title={Fair AutoML},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    author={Qingyun Wu and Chi Wang},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    year={2021},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    booktitle={ArXiv preprint arXiv:2111.06495},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">}</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">* [Mining Robust Default Configurations for Resource-constrained AutoML](https://arxiv.org/abs/2202.09927). Moe Kayali, Chi Wang. ArXiv preprint arXiv:2202.09927 (2022).</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```bibtex</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">@inproceedings{kayaliwang2022default,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    title={Mining Robust Default Configurations for Resource-constrained AutoML},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    author={Moe Kayali and Chi Wang},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    year={2022},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    booktitle={ArXiv preprint arXiv:2202.09927},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">}</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">* [Targeted Hyperparameter Optimization with Lexicographic Preferences Over Multiple Objectives](https://openreview.net/forum?id=0Ij9_q567Ma). Shaokun Zhang, Feiran Jia, Chi Wang, Qingyun Wu. ICLR 2023 (notable-top-5%).</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```bibtex</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">@inproceedings{zhang2023targeted,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    title={Targeted Hyperparameter Optimization with Lexicographic Preferences Over Multiple Objectives},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    author={Shaokun Zhang and Feiran Jia and Chi Wang and Qingyun Wu},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    booktitle={International Conference on Learning Representations},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    year={2023},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    url={https://openreview.net/forum?id=0Ij9_q567Ma},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">}</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">* [Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference](https://arxiv.org/abs/2303.04673). Chi Wang, Susan Xueqing Liu, Ahmed H. Awadallah. ArXiv preprint arXiv:2303.04673 (2023).</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```bibtex</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">@inproceedings{wang2023EcoOptiGen,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    title={Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    author={Chi Wang and Susan Xueqing Liu and Ahmed H. Awadallah},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    year={2023},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    booktitle={ArXiv preprint arXiv:2303.04673},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">}</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">* [An Empirical Study on Challenging Math Problem Solving with GPT-4](https://arxiv.org/abs/2306.01337). Yiran Wu, Feiran Jia, Shaokun Zhang, Hangyu Li, Erkang Zhu, Yue Wang, Yin Tat Lee, Richard Peng, Qingyun Wu, Chi Wang. ArXiv preprint arXiv:2306.01337 (2023).</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```bibtex</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">@inproceedings{wu2023empirical,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    title={An Empirical Study on Challenging Math Problem Solving with GPT-4},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    author={Yiran Wu and Feiran Jia and Shaokun Zhang and Hangyu Li and Erkang Zhu and Yue Wang and Yin Tat Lee and Richard Peng and Qingyun Wu and Chi Wang},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    year={2023},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    booktitle={ArXiv preprint arXiv:2306.01337},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">}</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">When you submit a pull request, a CLA bot will automatically determine whether you need to provide</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">provided by the bot. You will only need to do this once across all repos using our CLA.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token punctuation" style="color:rgb(199, 146, 234)">--------------------------------------------------------------------------------</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token message-id class-name" style="color:rgb(255, 203, 107)">assistant</span><span class="token message-id"> </span><span class="token message-id punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token message-id keyword" style="font-style:italic">to</span><span class="token message-id"> </span><span class="token message-id class-name" style="color:rgb(255, 203, 107)">ragproxyagent</span><span class="token message-id punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token message-id">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Based on the context provided, which is about the FLAML library, there is no direct reference to a function specifically called `tune_automl`. However, FLAML does offer functionality for automated machine learning (AutoML) and hyperparameter tuning.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">The closest reference to an AutoML tuning operation in the given context is shown in the Quickstart section, which demonstrates how to use FLAML as a scikit-learn style estimator for machine learning tasks like classification and regression. It does talk about automated machine learning and tuning, but doesn&#x27;t mention a function `tune_automl` by name.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">If you are looking for a way to perform tuning with FLAML, the context indicates you can use the `tune` module to run generic hyperparameter tuning for a custom function, as demonstrated in the Quickstart section:</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```python</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">from flaml import tune</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">tune.run(evaluation_function, config={…}, low_cost_partial_config={…}, time_budget_s=3600)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">This is not called `tune_automl` but rather just `tune.run`.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">If you need confirmation on whether a function called `tune_automl` specifically exists, the FLAML documentation or its API reference should be checked. If documentation is not enough to confirm and you require to look into the actual code or a structured list of functionalities provided by FLAML, that information isn&#x27;t available in the given context.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">In that case, the instruction should be: `UPDATE CONTEXT`.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token punctuation" style="color:rgb(199, 146, 234)">--------------------------------------------------------------------------------</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Updating context and resetting conversation.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">No more context, will terminate.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token message-id class-name" style="color:rgb(255, 203, 107)">ragproxyagent</span><span class="token message-id"> </span><span class="token message-id punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token message-id keyword" style="font-style:italic">to</span><span class="token message-id"> </span><span class="token message-id class-name" style="color:rgb(255, 203, 107)">assistant</span><span class="token message-id punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token message-id">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token builtin" style="color:rgb(130, 170, 255)">TERMINATE</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token punctuation" style="color:rgb(199, 146, 234)">--------------------------------------------------------------------------------</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token plain">ChatResult(chat_id=None, chat_history=[{&#x27;content&#x27;: &#x27;</span><span class="token builtin" style="color:rgb(130, 170, 255)">TERMINATE</span><span class="token plain">&#x27;, &#x27;role&#x27;: &#x27;assistant&#x27;}], summary=&#x27;&#x27;, cost=({&#x27;total_cost&#x27;: 0.12719999999999998, &#x27;gpt-4&#x27;: {&#x27;cost&#x27;: 0.12719999999999998, &#x27;prompt_tokens&#x27;: 3634, &#x27;completion_tokens&#x27;: 303, &#x27;total_tokens&#x27;: 3937}}, {&#x27;total_cost&#x27;: 0.12719999999999998, &#x27;gpt-4&#x27;: {&#x27;cost&#x27;: 0.12719999999999998, &#x27;prompt_tokens&#x27;: 3634, &#x27;completion_tokens&#x27;: 303, &#x27;total_tokens&#x27;: 3937}}), human_input=[])</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p><a id="example-2"></a> ### Example 2</p>
<p><a href="#toc">back to top</a></p>
<p>Use RetrieveChat to answer a question that is not related to code
generation.</p>
<p>Problem: Who is the author of FLAML?</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic"># reset the assistant. Always reset the assistant before starting a new conversation.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">assistant</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">reset</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">qa_problem </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;Who is the author of FLAML?&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">ragproxyagent</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">initiate_chat</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">assistant</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> message</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">ragproxyagent</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">message_generator</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> problem</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">qa_problem</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token plain">Adding doc_id 2 to context.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Adding doc_id 0 to context.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Adding doc_id 1 to context.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token message-id class-name" style="color:rgb(255, 203, 107)">ragproxyagent</span><span class="token message-id"> </span><span class="token message-id punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token message-id keyword" style="font-style:italic">to</span><span class="token message-id"> </span><span class="token message-id class-name" style="color:rgb(255, 203, 107)">assistant</span><span class="token message-id punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token message-id">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">You&#x27;re a retrieve augmented coding assistant. You answer user&#x27;s questions based on your own knowledge and the</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">context provided by the user.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">If you can&#x27;t answer the question with or without the current context, you should reply exactly `UPDATE CONTEXT`.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">For code generation, you must obey the following rules:</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Rule 1. You MUST NOT install any packages because all the packages needed are already installed.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Rule 2. You must follow the formats below to write your code:</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```language</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"># your code</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">User&#x27;s question is: Who is the author of FLAML?</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Context is: # Research</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">For technical details, please check our research publications.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">* [FLAML: A Fast and Lightweight AutoML Library](https://www.microsoft.com/en-us/research/publication/flaml-a-fast-and-lightweight-automl-library/). Chi Wang, Qingyun Wu, Markus Weimer, Erkang Zhu. MLSys 2021.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```bibtex</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">@inproceedings{wang2021flaml,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    title={FLAML: A Fast and Lightweight AutoML Library},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    author={Chi Wang and Qingyun Wu and Markus Weimer and Erkang Zhu},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    year={2021},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    booktitle={MLSys},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">}</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">* [Frugal Optimization for Cost-related Hyperparameters](https://arxiv.org/abs/2005.01571). Qingyun Wu, Chi Wang, Silu Huang. AAAI 2021.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```bibtex</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">@inproceedings{wu2021cfo,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    title={Frugal Optimization for Cost-related Hyperparameters},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    author={Qingyun Wu and Chi Wang and Silu Huang},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    year={2021},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    booktitle={AAAI},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">}</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">* [Economical Hyperparameter Optimization With Blended Search Strategy](https://www.microsoft.com/en-us/research/publication/economical-hyperparameter-optimization-with-blended-search-strategy/). Chi Wang, Qingyun Wu, Silu Huang, Amin Saied. ICLR 2021.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```bibtex</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">@inproceedings{wang2021blendsearch,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    title={Economical Hyperparameter Optimization With Blended Search Strategy},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    author={Chi Wang and Qingyun Wu and Silu Huang and Amin Saied},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    year={2021},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    booktitle={ICLR},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">}</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">* [An Empirical Study on Hyperparameter Optimization for Fine-Tuning Pre-trained Language Models](https://aclanthology.org/2021.acl-long.178.pdf). Susan Xueqing Liu, Chi Wang. ACL 2021.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```bibtex</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">@inproceedings{liuwang2021hpolm,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    title={An Empirical Study on Hyperparameter Optimization for Fine-Tuning Pre-trained Language Models},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    author={Susan Xueqing Liu and Chi Wang},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    year={2021},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    booktitle={ACL},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">}</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">* [ChaCha for Online AutoML](https://www.microsoft.com/en-us/research/publication/chacha-for-online-automl/). Qingyun Wu, Chi Wang, John Langford, Paul Mineiro and Marco Rossi. ICML 2021.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```bibtex</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">@inproceedings{wu2021chacha,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    title={ChaCha for Online AutoML},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    author={Qingyun Wu and Chi Wang and John Langford and Paul Mineiro and Marco Rossi},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    year={2021},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    booktitle={ICML},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">}</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">* [Fair AutoML](https://arxiv.org/abs/2111.06495). Qingyun Wu, Chi Wang. ArXiv preprint arXiv:2111.06495 (2021).</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```bibtex</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">@inproceedings{wuwang2021fairautoml,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    title={Fair AutoML},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    author={Qingyun Wu and Chi Wang},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    year={2021},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    booktitle={ArXiv preprint arXiv:2111.06495},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">}</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">* [Mining Robust Default Configurations for Resource-constrained AutoML](https://arxiv.org/abs/2202.09927). Moe Kayali, Chi Wang. ArXiv preprint arXiv:2202.09927 (2022).</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```bibtex</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">@inproceedings{kayaliwang2022default,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    title={Mining Robust Default Configurations for Resource-constrained AutoML},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    author={Moe Kayali and Chi Wang},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    year={2022},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    booktitle={ArXiv preprint arXiv:2202.09927},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">}</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">* [Targeted Hyperparameter Optimization with Lexicographic Preferences Over Multiple Objectives](https://openreview.net/forum?id=0Ij9_q567Ma). Shaokun Zhang, Feiran Jia, Chi Wang, Qingyun Wu. ICLR 2023 (notable-top-5%).</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```bibtex</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">@inproceedings{zhang2023targeted,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    title={Targeted Hyperparameter Optimization with Lexicographic Preferences Over Multiple Objectives},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    author={Shaokun Zhang and Feiran Jia and Chi Wang and Qingyun Wu},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    booktitle={International Conference on Learning Representations},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    year={2023},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    url={https://openreview.net/forum?id=0Ij9_q567Ma},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">}</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">* [Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference](https://arxiv.org/abs/2303.04673). Chi Wang, Susan Xueqing Liu, Ahmed H. Awadallah. ArXiv preprint arXiv:2303.04673 (2023).</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```bibtex</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">@inproceedings{wang2023EcoOptiGen,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    title={Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    author={Chi Wang and Susan Xueqing Liu and Ahmed H. Awadallah},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    year={2023},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    booktitle={ArXiv preprint arXiv:2303.04673},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">}</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">* [An Empirical Study on Challenging Math Problem Solving with GPT-4](https://arxiv.org/abs/2306.01337). Yiran Wu, Feiran Jia, Shaokun Zhang, Hangyu Li, Erkang Zhu, Yue Wang, Yin Tat Lee, Richard Peng, Qingyun Wu, Chi Wang. ArXiv preprint arXiv:2306.01337 (2023).</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```bibtex</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">@inproceedings{wu2023empirical,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    title={An Empirical Study on Challenging Math Problem Solving with GPT-4},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    author={Yiran Wu and Feiran Jia and Shaokun Zhang and Hangyu Li and Erkang Zhu and Yue Wang and Yin Tat Lee and Richard Peng and Qingyun Wu and Chi Wang},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    year={2023},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    booktitle={ArXiv preprint arXiv:2306.01337},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">}</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">[![PyPI version](https://badge.fury.io/py/FLAML.svg)](https://badge.fury.io/py/FLAML)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">![Conda version](https://img.shields.io/conda/vn/conda-forge/flaml)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">[![Build](https://github.com/microsoft/FLAML/actions/workflows/python-package.yml/badge.svg)](https://github.com/microsoft/FLAML/actions/workflows/python-package.yml)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">![Python Version](https://img.shields.io/badge/3.8%20%7C%203.9%20%7C%203.10-blue)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">[![Downloads](https://pepy.tech/badge/flaml)](https://pepy.tech/project/flaml)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">[![](https://img.shields.io/discord/1025786666260111483?logo=discord&amp;style=flat)](https://discord.gg/Cppx2vSPVP)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">&lt;!-- [![Join the chat at https://gitter.im/FLAMLer/community](https://badges.gitter.im/FLAMLer/community.svg)](https://gitter.im/FLAMLer/community?utm_source=badge&amp;utm_medium=badge&amp;utm_campaign=pr-badge&amp;utm_content=badge) --&gt;</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"># A Fast Library for Automated Machine Learning &amp; Tuning</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">&lt;p align=&quot;center&quot;&gt;</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    &lt;img src=&quot;https://github.com/microsoft/FLAML/blob/main/website/static/img/flaml.svg&quot;  width=200&gt;</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    &lt;br&gt;</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">&lt;/p&gt;</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">:fire: Heads-up: We have migrated [AutoGen](https://microsoft.github.io/autogen/) into a dedicated [github repository](https://github.com/microsoft/autogen). Alongside this move, we have also launched a dedicated [Discord](https://discord.gg/pAbnFJrkgZ) server and a [website](https://microsoft.github.io/autogen/) for comprehensive documentation.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">:fire: The automated multi-agent chat framework in [AutoGen](https://microsoft.github.io/autogen/) is in preview from v2.0.0.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">:fire: FLAML is highlighted in OpenAI&#x27;s [cookbook](https://github.com/openai/openai-cookbook#related-resources-from-around-the-web).</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">:fire: [autogen](https://microsoft.github.io/autogen/) is released with support for ChatGPT and GPT-4, based on [Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference](https://arxiv.org/abs/2303.04673).</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">:fire: FLAML supports Code-First AutoML &amp; Tuning – Private Preview in [Microsoft Fabric Data Science](https://learn.microsoft.com/en-us/fabric/data-science/).</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">## What is FLAML</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">FLAML is a lightweight Python library for efficient automation of machine</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">learning and AI operations. It automates workflow based on large language models, machine learning models, etc.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">and optimizes their performance.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">* FLAML enables building next-gen GPT-X applications based on multi-agent conversations with minimal effort. It simplifies the orchestration, automation and optimization of a complex GPT-X workflow. It maximizes the performance of GPT-X models and augments their weakness.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">* For common machine learning tasks like classification and regression, it quickly finds quality models for user-provided data with low computational resources. It is easy to customize or extend. Users can find their desired customizability from a smooth range.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">* It supports fast and economical automatic tuning (e.g., inference hyperparameters for foundation models, configurations in MLOps/LMOps workflows, pipelines, mathematical/statistical models, algorithms, computing experiments, software configurations), capable of handling large search space with heterogeneous evaluation cost and complex constraints/guidance/early stopping.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">FLAML is powered by a series of [research studies](https://microsoft.github.io/FLAML/docs/Research/) from Microsoft Research and collaborators such as Penn State University, Stevens Institute of Technology, University of Washington, and University of Waterloo.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">FLAML has a .NET implementation in [ML.NET](http://dot.net/ml), an open-source, cross-platform machine learning framework for .NET.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">## Installation</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">FLAML requires **Python version &gt;= 3.8**. It can be installed from pip:</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```bash</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">pip install flaml</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Minimal dependencies are installed without extra options. You can install extra options based on the feature you need. For example, use the following to install the dependencies needed by the [`autogen`](https://microsoft.github.io/autogen/) package.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```bash</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">pip install &quot;flaml[autogen]&quot;</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Find more options in [Installation](https://microsoft.github.io/FLAML/docs/Installation).</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Each of the [`notebook examples`](https://github.com/microsoft/FLAML/tree/main/notebook) may require a specific option to be installed.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">## Quickstart</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">* (New) The [autogen](https://microsoft.github.io/autogen/) package enables the next-gen GPT-X applications with a generic multi-agent conversation framework.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">It offers customizable and conversable agents which integrate LLMs, tools and human.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">By automating chat among multiple capable agents, one can easily make them collectively perform tasks autonomously or with human feedback, including tasks that require using tools via code. For example,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```python</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">from flaml import autogen</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">assistant = autogen.AssistantAgent(&quot;assistant&quot;)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">user_proxy = autogen.UserProxyAgent(&quot;user_proxy&quot;)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">user_proxy.initiate_chat(assistant, message=&quot;Show me the YTD gain of 10 largest technology companies as of today.&quot;)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"># This initiates an automated chat between the two agents to solve the task</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Autogen also helps maximize the utility out of the expensive LLMs such as ChatGPT and GPT-4. It offers a drop-in replacement of `openai.Completion` or `openai.ChatCompletion` with powerful functionalites like tuning, caching, templating, filtering. For example, you can optimize generations by LLM with your own tuning data, success metrics and budgets.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```python</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"># perform tuning</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">config, analysis = autogen.Completion.tune(</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    data=tune_data,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    metric=&quot;success&quot;,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    mode=&quot;max&quot;,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    eval_func=eval_func,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    inference_budget=0.05,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    optimization_budget=3,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    num_samples=-1,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"># perform inference for a test instance</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">response = autogen.Completion.create(context=test_instance, **config)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">* With three lines of code, you can start using this economical and fast</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">AutoML engine as a [scikit-learn style estimator](https://microsoft.github.io/FLAML/docs/Use-Cases/Task-Oriented-AutoML).</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```python</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">from flaml import AutoML</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">automl = AutoML()</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">automl.fit(X_train, y_train, task=&quot;classification&quot;)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">* You can restrict the learners and use FLAML as a fast hyperparameter tuning</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">tool for XGBoost, LightGBM, Random Forest etc. or a [customized learner](https://microsoft.github.io/FLAML/docs/Use-Cases/Task-Oriented-AutoML#estimator-and-search-space).</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```python</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">automl.fit(X_train, y_train, task=&quot;classification&quot;, estimator_list=[&quot;lgbm&quot;])</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">* You can also run generic hyperparameter tuning for a [custom function](https://microsoft.github.io/FLAML/docs/Use-Cases/Tune-User-Defined-Function).</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```python</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">from flaml import tune</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">tune.run(evaluation_function, config={…}, low_cost_partial_config={…}, time_budget_s=3600)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">* [Zero-shot AutoML](https://microsoft.github.io/FLAML/docs/Use-Cases/Zero-Shot-AutoML) allows using the existing training API from lightgbm, xgboost etc. while getting the benefit of AutoML in choosing high-performance hyperparameter configurations per task.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```python</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">from flaml.default import LGBMRegressor</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"># Use LGBMRegressor in the same way as you use lightgbm.LGBMRegressor.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">estimator = LGBMRegressor()</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"># The hyperparameters are automatically set according to the training data.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">estimator.fit(X_train, y_train)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">## Documentation</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">You can find a detailed documentation about FLAML [here](https://microsoft.github.io/FLAML/).</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">In addition, you can find:</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">- [Research](https://microsoft.github.io/FLAML/docs/Research) and [blogposts](https://microsoft.github.io/FLAML/blog) around FLAML.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">- [Discord](https://discord.gg/Cppx2vSPVP).</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">- [Contributing guide](https://microsoft.github.io/FLAML/docs/Contribute).</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">- ML.NET documentation and tutorials for [Model Builder](https://learn.microsoft.com/dotnet/machine-learning/tutorials/predict-prices-with-model-builder), [ML.NET CLI](https://learn.microsoft.com/dotnet/machine-learning/tutorials/sentiment-analysis-cli), and [AutoML API](https://learn.microsoft.com/dotnet/machine-learning/how-to-guides/how-to-use-the-automl-api).</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">## Contributing</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">This project welcomes contributions and suggestions. Most contributions require you to agree to a</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">the rights to use your contribution. For details, visit &lt;https://cla.opensource.microsoft.com&gt;.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">If you are new to GitHub [here](https://help.github.com/categories/collaborating-with-issues-and-pull-requests/) is a detailed help source on getting involved with development on GitHub.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">When you submit a pull request, a CLA bot will automatically determine whether you need to provide</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">provided by the bot. You will only need to do this once across all repos using our CLA.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token punctuation" style="color:rgb(199, 146, 234)">--------------------------------------------------------------------------------</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token message-id class-name" style="color:rgb(255, 203, 107)">assistant</span><span class="token message-id"> </span><span class="token message-id punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token message-id keyword" style="font-style:italic">to</span><span class="token message-id"> </span><span class="token message-id class-name" style="color:rgb(255, 203, 107)">ragproxyagent</span><span class="token message-id punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token message-id">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">The author of FLAML is Chi Wang, along with other collaborators including Qingyun Wu, Markus Weimer, Erkang Zhu, Silu Huang, Amin Saied, Susan Xueqing Liu, John Langford, Paul Mineiro, Marco Rossi, Moe Kayali, Shaokun Zhang, Feiran Jia, Yiran Wu, Hangyu Li, Yue Wang, Yin Tat Lee, Richard Peng, and Ahmed H. Awadallah, as indicated in the provided references for FLAML&#x27;s research publications.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token punctuation" style="color:rgb(199, 146, 234)">--------------------------------------------------------------------------------</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token plain">ChatResult(chat_id=None, chat_history=[{&#x27;content&#x27;: &#x27;You\&#x27;re a retrieve augmented coding assistant. You answer user\&#x27;s questions based on your own knowledge and the\ncontext provided by the user.\nIf you can\&#x27;t answer the question with or without the current context, you should reply exactly `UPDATE CONTEXT`.\nFor code generation, you must obey the following rules:\nRule 1. You MUST NOT install any packages because all the packages needed are already installed.\nRule 2. You must follow the formats below to write your code:\n```language\n# your code\n```\n\nUser\&#x27;s question is: Who is the author of FLAML?\n\nContext is: # Research\n\nFor technical details, please check our research publications.\n\n* [FLAML: A Fast and Lightweight AutoML Library](https://www.microsoft.com/en-us/research/publication/flaml-a-fast-and-lightweight-automl-library/). Chi Wang, Qingyun Wu, Markus Weimer, Erkang Zhu. MLSys 2021.\n\n```bibtex\n@inproceedings{wang2021flaml,\n    title={FLAML: A Fast and Lightweight AutoML Library},\n    author={Chi Wang and Qingyun Wu and Markus Weimer and Erkang Zhu},\n    year={2021},\n    booktitle={MLSys},\n}\n```\n\n* [Frugal Optimization for Cost-related Hyperparameters](https://arxiv.org/abs/2005.01571). Qingyun Wu, Chi Wang, Silu Huang. AAAI 2021.\n\n```bibtex\n@inproceedings{wu2021cfo,\n    title={Frugal Optimization for Cost-related Hyperparameters},\n    author={Qingyun Wu and Chi Wang and Silu Huang},\n    year={2021},\n    booktitle={AAAI},\n}\n```\n\n* [Economical Hyperparameter Optimization With Blended Search Strategy](https://www.microsoft.com/en-us/research/publication/economical-hyperparameter-optimization-with-blended-search-strategy/). Chi Wang, Qingyun Wu, Silu Huang, Amin Saied. ICLR 2021.\n\n```bibtex\n@inproceedings{wang2021blendsearch,\n    title={Economical Hyperparameter Optimization With Blended Search Strategy},\n    author={Chi Wang and Qingyun Wu and Silu Huang and Amin Saied},\n    year={2021},\n    booktitle={ICLR},\n}\n```\n\n* [An Empirical Study on Hyperparameter Optimization for Fine-Tuning Pre-trained Language Models](https://aclanthology.org/2021.acl-long.178.pdf). Susan Xueqing Liu, Chi Wang. ACL 2021.\n\n```bibtex\n@inproceedings{liuwang2021hpolm,\n    title={An Empirical Study on Hyperparameter Optimization for Fine-Tuning Pre-trained Language Models},\n    author={Susan Xueqing Liu and Chi Wang},\n    year={2021},\n    booktitle={ACL},\n}\n```\n\n* [ChaCha for Online AutoML](https://www.microsoft.com/en-us/research/publication/chacha-for-online-automl/). Qingyun Wu, Chi Wang, John Langford, Paul Mineiro and Marco Rossi. ICML 2021.\n\n```bibtex\n@inproceedings{wu2021chacha,\n    title={ChaCha for Online AutoML},\n    author={Qingyun Wu and Chi Wang and John Langford and Paul Mineiro and Marco Rossi},\n    year={2021},\n    booktitle={ICML},\n}\n```\n\n* [Fair AutoML](https://arxiv.org/abs/2111.06495). Qingyun Wu, Chi Wang. ArXiv preprint arXiv:2111.06495 (2021).\n\n```bibtex\n@inproceedings{wuwang2021fairautoml,\n    title={Fair AutoML},\n    author={Qingyun Wu and Chi Wang},\n    year={2021},\n    booktitle={ArXiv preprint arXiv:2111.06495},\n}\n```\n\n* [Mining Robust Default Configurations for Resource-constrained AutoML](https://arxiv.org/abs/2202.09927). Moe Kayali, Chi Wang. ArXiv preprint arXiv:2202.09927 (2022).\n\n```bibtex\n@inproceedings{kayaliwang2022default,\n    title={Mining Robust Default Configurations for Resource-constrained AutoML},\n    author={Moe Kayali and Chi Wang},\n    year={2022},\n    booktitle={ArXiv preprint arXiv:2202.09927},\n}\n```\n\n* [Targeted Hyperparameter Optimization with Lexicographic Preferences Over Multiple Objectives](https://openreview.net/forum?id=0Ij9_q567Ma). Shaokun Zhang, Feiran Jia, Chi Wang, Qingyun Wu. ICLR 2023 (notable-top-5%).\n\n```bibtex\n@inproceedings{zhang2023targeted,\n    title={Targeted Hyperparameter Optimization with Lexicographic Preferences Over Multiple Objectives},\n    author={Shaokun Zhang and Feiran Jia and Chi Wang and Qingyun Wu},\n    booktitle={International Conference on Learning Representations},\n    year={2023},\n    url={https://openreview.net/forum?id=0Ij9_q567Ma},\n}\n```\n\n* [Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference](https://arxiv.org/abs/2303.04673). Chi Wang, Susan Xueqing Liu, Ahmed H. Awadallah. ArXiv preprint arXiv:2303.04673 (2023).\n\n```bibtex\n@inproceedings{wang2023EcoOptiGen,\n    title={Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference},\n    author={Chi Wang and Susan Xueqing Liu and Ahmed H. Awadallah},\n    year={2023},\n    booktitle={ArXiv preprint arXiv:2303.04673},\n}\n```\n\n* [An Empirical Study on Challenging Math Problem Solving with GPT-4](https://arxiv.org/abs/2306.01337). Yiran Wu, Feiran Jia, Shaokun Zhang, Hangyu Li, Erkang Zhu, Yue Wang, Yin Tat Lee, Richard Peng, Qingyun Wu, Chi Wang. ArXiv preprint arXiv:2306.01337 (2023).\n\n```bibtex\n@inproceedings{wu2023empirical,\n    title={An Empirical Study on Challenging Math Problem Solving with GPT-4},\n    author={Yiran Wu and Feiran Jia and Shaokun Zhang and Hangyu Li and Erkang Zhu and Yue Wang and Yin Tat Lee and Richard Peng and Qingyun Wu and Chi Wang},\n    year={2023},\n    booktitle={ArXiv preprint arXiv:2306.01337},\n}\n```\n\n[![PyPI version](https://badge.fury.io/py/FLAML.svg)](https://badge.fury.io/py/FLAML)\n![Conda version](https://img.shields.io/conda/vn/conda-forge/flaml)\n[![Build](https://github.com/microsoft/FLAML/actions/workflows/python-package.yml/badge.svg)](https://github.com/microsoft/FLAML/actions/workflows/python-package.yml)\n![Python Version](https://img.shields.io/badge/3.8%20%7C%203.9%20%7C%203.10-blue)\n[![Downloads](https://pepy.tech/badge/flaml)](https://pepy.tech/project/flaml)\n[![](https://img.shields.io/discord/1025786666260111483?logo=discord&amp;style=flat)](https://discord.gg/Cppx2vSPVP)\n&lt;!-- [![Join the chat at https://gitter.im/FLAMLer/community](https://badges.gitter.im/FLAMLer/community.svg)](https://gitter.im/FLAMLer/community?utm_source=badge&amp;utm_medium=badge&amp;utm_campaign=pr-badge&amp;utm_content=badge) --&gt;\n\n\n# A Fast Library for Automated Machine Learning &amp; Tuning\n\n&lt;p align=&quot;center&quot;&gt;\n    &lt;img src=&quot;https://github.com/microsoft/FLAML/blob/main/website/static/img/flaml.svg&quot;  width=200&gt;\n    &lt;br&gt;\n&lt;/p&gt;\n\n:fire: Heads-up: We have migrated [AutoGen](https://microsoft.github.io/autogen/) into a dedicated [github repository](https://github.com/microsoft/autogen). Alongside this move, we have also launched a dedicated [Discord](https://discord.gg/pAbnFJrkgZ) server and a [website](https://microsoft.github.io/autogen/) for comprehensive documentation.\n\n:fire: The automated multi-agent chat framework in [AutoGen](https://microsoft.github.io/autogen/) is in preview from v2.0.0.\n\n:fire: FLAML is highlighted in OpenAI\&#x27;s [cookbook](https://github.com/openai/openai-cookbook#related-resources-from-around-the-web).\n\n:fire: [autogen](https://microsoft.github.io/autogen/) is released with support for ChatGPT and GPT-4, based on [Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference](https://arxiv.org/abs/2303.04673).\n\n:fire: FLAML supports Code-First AutoML &amp; Tuning – Private Preview in [Microsoft Fabric Data Science](https://learn.microsoft.com/en-us/fabric/data-science/).\n\n\n## What is FLAML\nFLAML is a lightweight Python library for efficient automation of machine\nlearning and AI operations. It automates workflow based on large language models, machine learning models, etc.\nand optimizes their performance.\n\n* FLAML enables building next-gen GPT-X applications based on multi-agent conversations with minimal effort. It simplifies the orchestration, automation and optimization of a complex GPT-X workflow. It maximizes the performance of GPT-X models and augments their weakness.\n* For common machine learning tasks like classification and regression, it quickly finds quality models for user-provided data with low computational resources. It is easy to customize or extend. Users can find their desired customizability from a smooth range.\n* It supports fast and economical automatic tuning (e.g., inference hyperparameters for foundation models, configurations in MLOps/LMOps workflows, pipelines, mathematical/statistical models, algorithms, computing experiments, software configurations), capable of handling large search space with heterogeneous evaluation cost and complex constraints/guidance/early stopping.\n\nFLAML is powered by a series of [research studies](https://microsoft.github.io/FLAML/docs/Research/) from Microsoft Research and collaborators such as Penn State University, Stevens Institute of Technology, University of Washington, and University of Waterloo.\n\nFLAML has a .NET implementation in [ML.NET](http://dot.net/ml), an open-source, cross-platform machine learning framework for .NET.\n\n## Installation\n\nFLAML requires **Python version &gt;= 3.8**. It can be installed from pip:\n\n```bash\npip install flaml\n```\n\nMinimal dependencies are installed without extra options. You can install extra options based on the feature you need. For example, use the following to install the dependencies needed by the [`autogen`](https://microsoft.github.io/autogen/) package.\n```bash\npip install &quot;flaml[autogen]&quot;\n```\n\nFind more options in [Installation](https://microsoft.github.io/FLAML/docs/Installation).\nEach of the [`notebook examples`](https://github.com/microsoft/FLAML/tree/main/notebook) may require a specific option to be installed.\n\n## Quickstart\n\n* (New) The [autogen](https://microsoft.github.io/autogen/) package enables the next-gen GPT-X applications with a generic multi-agent conversation framework.\nIt offers customizable and conversable agents which integrate LLMs, tools and human.\nBy automating chat among multiple capable agents, one can easily make them collectively perform tasks autonomously or with human feedback, including tasks that require using tools via code. For example,\n```python\nfrom flaml import autogen\nassistant = autogen.AssistantAgent(&quot;assistant&quot;)\nuser_proxy = autogen.UserProxyAgent(&quot;user_proxy&quot;)\nuser_proxy.initiate_chat(assistant, message=&quot;Show me the YTD gain of 10 largest technology companies as of today.&quot;)\n# This initiates an automated chat between the two agents to solve the task\n```\n\nAutogen also helps maximize the utility out of the expensive LLMs such as ChatGPT and GPT-4. It offers a drop-in replacement of `openai.Completion` or `openai.ChatCompletion` with powerful functionalites like tuning, caching, templating, filtering. For example, you can optimize generations by LLM with your own tuning data, success metrics and budgets.\n```python\n# perform tuning\nconfig, analysis = autogen.Completion.tune(\n    data=tune_data,\n    metric=&quot;success&quot;,\n    mode=&quot;max&quot;,\n    eval_func=eval_func,\n    inference_budget=0.05,\n    optimization_budget=3,\n    num_samples=-1,\n)\n# perform inference for a test instance\nresponse = autogen.Completion.create(context=test_instance, **config)\n```\n* With three lines of code, you can start using this economical and fast\nAutoML engine as a [scikit-learn style estimator](https://microsoft.github.io/FLAML/docs/Use-Cases/Task-Oriented-AutoML).\n\n```python\nfrom flaml import AutoML\nautoml = AutoML()\nautoml.fit(X_train, y_train, task=&quot;classification&quot;)\n```\n\n* You can restrict the learners and use FLAML as a fast hyperparameter tuning\ntool for XGBoost, LightGBM, Random Forest etc. or a [customized learner](https://microsoft.github.io/FLAML/docs/Use-Cases/Task-Oriented-AutoML#estimator-and-search-space).\n\n```python\nautoml.fit(X_train, y_train, task=&quot;classification&quot;, estimator_list=[&quot;lgbm&quot;])\n```\n\n* You can also run generic hyperparameter tuning for a [custom function](https://microsoft.github.io/FLAML/docs/Use-Cases/Tune-User-Defined-Function).\n\n```python\nfrom flaml import tune\ntune.run(evaluation_function, config={…}, low_cost_partial_config={…}, time_budget_s=3600)\n```\n\n* [Zero-shot AutoML](https://microsoft.github.io/FLAML/docs/Use-Cases/Zero-Shot-AutoML) allows using the existing training API from lightgbm, xgboost etc. while getting the benefit of AutoML in choosing high-performance hyperparameter configurations per task.\n\n```python\nfrom flaml.default import LGBMRegressor\n\n# Use LGBMRegressor in the same way as you use lightgbm.LGBMRegressor.\nestimator = LGBMRegressor()\n# The hyperparameters are automatically set according to the training data.\nestimator.fit(X_train, y_train)\n```\n\n## Documentation\n\nYou can find a detailed documentation about FLAML [here](https://microsoft.github.io/FLAML/).\n\nIn addition, you can find:\n\n- [Research](https://microsoft.github.io/FLAML/docs/Research) and [blogposts](https://microsoft.github.io/FLAML/blog) around FLAML.\n\n- [Discord](https://discord.gg/Cppx2vSPVP).\n\n- [Contributing guide](https://microsoft.github.io/FLAML/docs/Contribute).\n\n- ML.NET documentation and tutorials for [Model Builder](https://learn.microsoft.com/dotnet/machine-learning/tutorials/predict-prices-with-model-builder), [ML.NET CLI](https://learn.microsoft.com/dotnet/machine-learning/tutorials/sentiment-analysis-cli), and [AutoML API](https://learn.microsoft.com/dotnet/machine-learning/how-to-guides/how-to-use-the-automl-api).\n\n## Contributing\n\nThis project welcomes contributions and suggestions. Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit &lt;https://cla.opensource.microsoft.com&gt;.\n\nIf you are new to GitHub [here](https://help.github.com/categories/collaborating-with-issues-and-pull-requests/) is a detailed help source on getting involved with development on GitHub.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n\n&#x27;, &#x27;role&#x27;: &#x27;assistant&#x27;}, {&#x27;content&#x27;: &quot;The author of FLAML is Chi Wang, along with other collaborators including Qingyun Wu, Markus Weimer, Erkang Zhu, Silu Huang, Amin Saied, Susan Xueqing Liu, John Langford, Paul Mineiro, Marco Rossi, Moe Kayali, Shaokun Zhang, Feiran Jia, Yiran Wu, Hangyu Li, Yue Wang, Yin Tat Lee, Richard Peng, and Ahmed H. Awadallah, as indicated in the provided references for FLAML&#x27;s research publications.&quot;, &#x27;role&#x27;: &#x27;user&#x27;}], summary=&quot;The author of FLAML is Chi Wang, along with other collaborators including Qingyun Wu, Markus Weimer, Erkang Zhu, Silu Huang, Amin Saied, Susan Xueqing Liu, John Langford, Paul Mineiro, Marco Rossi, Moe Kayali, Shaokun Zhang, Feiran Jia, Yiran Wu, Hangyu Li, Yue Wang, Yin Tat Lee, Richard Peng, and Ahmed H. Awadallah, as indicated in the provided references for FLAML&#x27;s research publications.&quot;, cost=({&#x27;total_cost&#x27;: 0.11538, &#x27;gpt-4&#x27;: {&#x27;cost&#x27;: 0.11538, &#x27;prompt_tokens&#x27;: 3632, &#x27;completion_tokens&#x27;: 107, &#x27;total_tokens&#x27;: 3739}}, {&#x27;total_cost&#x27;: 0}), human_input=[])</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-tags-row row margin-bottom--sm"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/autogen/docs/tags/rag">rag</a></li></ul></div></div><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/microsoft/autogen/edit/main/notebook/agentchat_qdrant_RetrieveChat.ipynb" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/autogen/docs/notebooks/agentchat_nestedchat_optiguide"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">OptiGuide with Nested Chats in AutoGen</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/autogen/docs/notebooks/agentchat_society_of_mind"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">SocietyOfMindAgent</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#set-your-api-endpoint" class="table-of-contents__link toc-highlight">Set your API Endpoint</a></li><li><a href="#construct-agents-for-retrievechat" class="table-of-contents__link toc-highlight">Construct agents for RetrieveChat</a><ul><li><a href="#you-can-find-the-list-of-all-the-embedding-models-supported-by-qdrant-here." class="table-of-contents__link toc-highlight">You can find the list of all the embedding models supported by Qdrant here.</a></li></ul></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://discord.gg/pAbnFJrkgZ" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/pyautogen" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024 AutoGen Authors |  <a target="_blank" style="color:#10adff" href="https://go.microsoft.com/fwlink/?LinkId=521839">Privacy and Cookies</a></div></div></div></footer></div>
</body>
</html>